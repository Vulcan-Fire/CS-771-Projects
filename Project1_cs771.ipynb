{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**TEMPLATE BELOW**"
      ],
      "metadata": {
        "id": "MsC_acv7pWT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# these are dummy models\n",
        "class MLModel():\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def train(self, X, y):\n",
        "        NotImplemented\n",
        "\n",
        "    def predict(self, X):\n",
        "        NotImplemented\n",
        "\n",
        "class TextSeqModel(MLModel):\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):# random predictions\n",
        "        return np.random.randint(0,2,(len(X)))\n",
        "\n",
        "\n",
        "class EmoticonModel(MLModel):\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):# random predictions\n",
        "        return np.random.randint(0,2,(len(X)))\n",
        "\n",
        "class FeatureModel(MLModel):\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def predict(self, X): # random predictions\n",
        "        return np.random.randint(0,2,(len(X)))\n",
        "\n",
        "class CombinedModel(MLModel):\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def predict(self, X1, X2, X3): # random predictions\n",
        "        return np.random.randint(0,2,(len(X1)))\n",
        "\n",
        "\n",
        "def save_predictions_to_file(predictions, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        for pred in predictions:\n",
        "            f.write(f\"{pred}\\n\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # read datasets\n",
        "    test_feat_X = np.load(\"datasets/test/test_feature.npz\", allow_pickle=True)['features']\n",
        "    test_emoticon_X = pd.read_csv(\"datasets/test/test_emoticon.csv\")['input_emoticon'].tolist()\n",
        "    test_seq_X = pd.read_csv(\"datasets/test/test_text_seq.csv\")['input_str'].tolist()\n",
        "\n",
        "    # your trained models\n",
        "    feature_model = FeatureModel()\n",
        "    text_model = TextSeqModel()\n",
        "    emoticon_model  = EmoticonModel()\n",
        "    best_model = CombinedModel()\n",
        "\n",
        "    # predictions from your trained models\n",
        "    pred_feat = feature_model.predict(test_feat_X)\n",
        "    pred_emoticons = emoticon_model.predict(test_emoticon_X)\n",
        "    pred_text = text_model.predict(test_seq_X)\n",
        "    pred_combined = best_model.predict(test_feat_X, test_emoticon_X, test_seq_X)\n",
        "\n",
        "    # saving prediction to text files\n",
        "    save_predictions_to_file(pred_feat, \"pred_feat.txt\")\n",
        "    save_predictions_to_file(pred_emoticons, \"pred_emoticon.txt\")\n",
        "    save_predictions_to_file(pred_text, \"pred_text.txt\")\n",
        "    save_predictions_to_file(pred_combined, \"pred_combined.txt\")\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "1ojO09lEpR8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dMKbSN9wVl8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2c6190-2d58-4093-e90a-297221a38750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import joblib  # For saving the models\n"
      ],
      "metadata": {
        "id": "oiHllUi97daY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "def load_data():\n",
        "    # Load Emoticon Dataset\n",
        "    train_emoticon_df = pd.read_csv(\"/content/drive/MyDrive/datasets/train/train_emoticon.csv\")\n",
        "    train_emoticon_X = train_emoticon_df['input_emoticon'].tolist()\n",
        "    train_emoticon_Y = train_emoticon_df['label'].tolist()\n",
        "\n",
        "    # Load Text Sequence Dataset\n",
        "    train_seq_df = pd.read_csv(\"/content/drive/MyDrive/datasets/train/train_text_seq.csv\")\n",
        "    train_seq_X = train_seq_df['input_str'].tolist()  # Keep input_str as string to preserve precision\n",
        "    train_seq_Y = train_seq_df['label'].tolist()\n",
        "\n",
        "    # Load Features Dataset\n",
        "    train_feat = np.load(\"/content/drive/MyDrive/datasets/train/train_feature.npz\", allow_pickle=True)\n",
        "    train_feat_X = train_feat['features']  # Assuming features are already in usable format\n",
        "    train_feat_Y = train_feat['label']\n",
        "\n",
        "    # Load Validation datasets similarly\n",
        "    val_emoticon_df = pd.read_csv(\"/content/drive/MyDrive/datasets/valid/valid_emoticon.csv\")\n",
        "    val_emoticon_X = val_emoticon_df['input_emoticon'].tolist()\n",
        "    val_emoticon_Y = val_emoticon_df['label'].tolist()\n",
        "\n",
        "    val_seq_df = pd.read_csv(\"/content/drive/MyDrive/datasets/valid/valid_text_seq.csv\")\n",
        "    val_seq_X = val_seq_df['input_str'].tolist()  # Keep input_str as string\n",
        "    val_seq_Y = val_seq_df['label'].tolist()\n",
        "\n",
        "    val_feat = np.load(\"/content/drive/MyDrive/datasets/valid/valid_feature.npz\", allow_pickle=True)\n",
        "    val_feat_X = val_feat['features']  # This should also be in 3D\n",
        "    val_feat_Y = val_feat['label']\n",
        "\n",
        "    return (train_emoticon_X, train_emoticon_Y), (train_seq_X, train_seq_Y), (train_feat_X, train_feat_Y), \\\n",
        "           (val_emoticon_X, val_emoticon_Y), (val_seq_X, val_seq_Y), (val_feat_X, val_feat_Y)\n"
      ],
      "metadata": {
        "id": "DwgCSmoa7oRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INFERIOR MODEL BELOW"
      ],
      "metadata": {
        "id": "_WYkmanjpmaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_emoticon_features(emoticon_list, encoder=None):\n",
        "    if encoder is None:\n",
        "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # One-hot encoding\n",
        "        emoticon_encoded = encoder.fit_transform(np.array(emoticon_list).reshape(-1, 1))\n",
        "    else:\n",
        "        emoticon_encoded = encoder.transform(np.array(emoticon_list).reshape(-1, 1))\n",
        "    return emoticon_encoded, encoder"
      ],
      "metadata": {
        "id": "K7c7rmJC7v0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, X_train, y_train, X_val, y_val):\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, predictions)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "Cc-OHlu18L3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    (train_emoticon_X, train_emoticon_Y), (train_seq_X, train_seq_Y), (train_feat_X, train_feat_Y), \\\n",
        "    (val_emoticon_X, val_emoticon_Y), (val_seq_X, val_seq_Y), (val_feat_X, val_feat_Y) = load_data()\n",
        "\n",
        "    # Extract features for emoticons\n",
        "    train_emoticon_X_encoded, encoder = extract_emoticon_features(train_emoticon_X)\n",
        "    val_emoticon_X_encoded, _ = extract_emoticon_features(val_emoticon_X, encoder)\n",
        "\n",
        "    # Initialize models\n",
        "    lr_model = LogisticRegression(max_iter=1000)\n",
        "    svm_model = SVC()\n",
        "    dt_model = DecisionTreeClassifier()\n",
        "\n",
        "    # Train models and evaluate\n",
        "    emoticon_accuracy = train_and_evaluate(lr_model, train_emoticon_X_encoded, train_emoticon_Y,\n",
        "                                           val_emoticon_X_encoded, val_emoticon_Y)\n",
        "\n",
        "    seq_accuracy = train_and_evaluate(svm_model, np.array(train_seq_X).reshape(-1, 1), train_seq_Y,\n",
        "                                       np.array(val_seq_X).reshape(-1, 1), val_seq_Y)\n",
        "\n",
        "    if train_feat_X.ndim == 3:\n",
        "        train_feat_X = train_feat_X.reshape(train_feat_X.shape[0], -1)\n",
        "\n",
        "    # val_feat = np.load(\"datasets/valid/valid_feature.npz\", allow_pickle=True)\n",
        "    # val_feat_X = val_feat['features']\n",
        "    # val_feat_Y = val_feat['label']\n",
        "\n",
        "    if val_feat_X.ndim == 3:\n",
        "        val_feat_X = val_feat_X.reshape(val_feat_X.shape[0], -1)\n",
        "\n",
        "    feat_accuracy = train_and_evaluate(dt_model, train_feat_X, train_feat_Y,\n",
        "                                       val_feat_X, val_feat_Y)\n",
        "\n",
        "    # Define the directory where you want to save the models\n",
        "    save_dir = '/content/drive/MyDrive/Task1 Models/Full/'\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    # Save models to the specified directory in Google Drive\n",
        "    joblib.dump(lr_model, os.path.join(save_dir, 'emoticon_model.pkl'))\n",
        "    joblib.dump(svm_model, os.path.join(save_dir, 'text_seq_model.pkl'))\n",
        "    joblib.dump(dt_model, os.path.join(save_dir, 'feature_model.pkl'))\n",
        "\n",
        "    # Print the accuracies\n",
        "    print(f\"Emoticon Model (LR) Accuracy: {emoticon_accuracy}\")\n",
        "    print(f\"Text Sequence Model (SVM) Accuracy: {seq_accuracy}\")\n",
        "    print(f\"Feature Model (Decision Tree) Accuracy: {feat_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8LcGQcp8NFi",
        "outputId": "10960dd1-e446-4494-930a-9b3ba24327f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emoticon Model (LR) Accuracy: 0.5153374233128835\n",
            "Text Sequence Model (SVM) Accuracy: 0.5255623721881391\n",
            "Feature Model (Decision Tree) Accuracy: 0.9631901840490797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOTHER INFERIOR APPROACH"
      ],
      "metadata": {
        "id": "9nxDkQ9rp3ZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Assuming you have load_data(), get_data_split(), preprocess_emoticon_data(), and train_and_evaluate() functions\n",
        "def handle_invalid_values(data):\n",
        "    data = np.array(data)\n",
        "    # Replace infinities and NaNs with zero or any other strategy\n",
        "    data[np.isinf(data)] = 0\n",
        "    data[np.isnan(data)] = 0\n",
        "    return data\n",
        "\n",
        "def preprocess_emoticon_data(data, encoder=None):\n",
        "    # Reshape the data to be 2D (each sample as a row)\n",
        "    data = np.array(data).reshape(-1, 1)\n",
        "\n",
        "    if encoder is None:\n",
        "        encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "        data_encoded = encoder.fit_transform(data).toarray()\n",
        "    else:\n",
        "        data_encoded = encoder.transform(data).toarray()\n",
        "\n",
        "    return data_encoded, encoder\n",
        "\n",
        "def scale_data(train_data, val_data):\n",
        "    scaler = StandardScaler()\n",
        "    train_data_scaled = scaler.fit_transform(train_data)\n",
        "    val_data_scaled = scaler.transform(val_data)\n",
        "    return train_data_scaled, val_data_scaled\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load data\n",
        "    (train_emoticon_X, train_emoticon_Y), (train_seq_X, train_seq_Y), (train_feat_X, train_feat_Y), \\\n",
        "    (val_emoticon_X, val_emoticon_Y), (val_seq_X, val_seq_Y), (val_feat_X, val_feat_Y) = load_data()\n",
        "\n",
        "    # Define splits\n",
        "    splits = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "\n",
        "    # Initialize encoder for emoticons\n",
        "    encoder = None\n",
        "\n",
        "    # Define models for all three datasets\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "        \"SVM\": SVC(),\n",
        "        \"Decision Tree\": DecisionTreeClassifier()\n",
        "    }\n",
        "\n",
        "    for split in splits:\n",
        "        print(f\"Training with {int(split * 100)}% of the data...\")\n",
        "\n",
        "        # ------------------- Emoticon Data -------------------\n",
        "        # Split and preprocess emoticon data\n",
        "        train_emoticon_X_split, train_emoticon_Y_split = get_data_split(train_emoticon_X, train_emoticon_Y, split)\n",
        "        train_emoticon_X_encoded, encoder = preprocess_emoticon_data(train_emoticon_X_split, encoder)\n",
        "        val_emoticon_X_encoded, _ = preprocess_emoticon_data(val_emoticon_X, encoder)\n",
        "\n",
        "        # Train and evaluate models on emoticon data\n",
        "        accuracies = {\"Emoticon\": {}, \"Text Sequence\": {}, \"Feature\": {}}\n",
        "        for model_name, model in models.items():\n",
        "            accuracy = train_and_evaluate(model, train_emoticon_X_encoded, train_emoticon_Y_split,\n",
        "                                          val_emoticon_X_encoded, val_emoticon_Y)\n",
        "            accuracies[\"Emoticon\"][model_name] = accuracy\n",
        "\n",
        "        # ------------------- Text Sequence Data -------------------\n",
        "        # Split and preprocess text sequence data\n",
        "        train_seq_X_split, train_seq_Y_split = get_data_split(train_seq_X, train_seq_Y, split)\n",
        "        train_seq_X_split = handle_invalid_values(train_seq_X_split)\n",
        "        val_seq_X = handle_invalid_values(val_seq_X)\n",
        "\n",
        "        # Scale sequence data\n",
        "        train_seq_X_scaled, val_seq_X_scaled = scale_data(np.array(train_seq_X_split).reshape(-1, 1), np.array(val_seq_X).reshape(-1, 1))\n",
        "\n",
        "        # Train models on text sequence data\n",
        "        for model_name, model in models.items():\n",
        "            accuracy = train_and_evaluate(model, train_seq_X_scaled, train_seq_Y_split, val_seq_X_scaled, val_seq_Y)\n",
        "            accuracies[\"Text Sequence\"][model_name] = accuracy\n",
        "\n",
        "        # ------------------- Feature Data -------------------\n",
        "        # Reshape features if 3D and split\n",
        "        if train_feat_X.ndim == 3:\n",
        "            train_feat_X = train_feat_X.reshape(train_feat_X.shape[0], -1)\n",
        "        if val_feat_X.ndim == 3:\n",
        "            val_feat_X = val_feat_X.reshape(val_feat_X.shape[0], -1)\n",
        "\n",
        "        train_feat_X_split, train_feat_Y_split = get_data_split(train_feat_X, train_feat_Y, split)\n",
        "\n",
        "        # Handle invalid values in feature data\n",
        "        train_feat_X_split = handle_invalid_values(train_feat_X_split)\n",
        "        val_feat_X = handle_invalid_values(val_feat_X)\n",
        "\n",
        "        # Scale feature data\n",
        "        train_feat_X_scaled, val_feat_X_scaled = scale_data(train_feat_X_split, val_feat_X)\n",
        "\n",
        "        # Train models on feature data\n",
        "        for model_name, model in models.items():\n",
        "            accuracy = train_and_evaluate(model, train_feat_X_scaled, train_feat_Y_split, val_feat_X_scaled, val_feat_Y)\n",
        "            accuracies[\"Feature\"][model_name] = accuracy\n",
        "\n",
        "        # Save models for each split\n",
        "        model_save_path = f'/content/Task1 Models/{int(split * 100)}'\n",
        "        os.makedirs(model_save_path, exist_ok=True)  # Create directory if it doesn't exist\n",
        "\n",
        "        # Save all models (for emoticon, text sequence, and feature data)\n",
        "        for model_name, model in models.items():\n",
        "            joblib.dump(model, f'{model_save_path}/emoticon_model_{model_name}_{int(split * 100)}.pkl')\n",
        "            joblib.dump(model, f'{model_save_path}/text_seq_model_{model_name}_{int(split * 100)}.pkl')\n",
        "            joblib.dump(model, f'{model_save_path}/feature_model_{model_name}_{int(split * 100)}.pkl')\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Results with {int(split * 100)}% of the data:\")\n",
        "        print(f\"Emoticon Model Accuracies: {accuracies['Emoticon']}\")\n",
        "        print(f\"Text Sequence Model Accuracies: {accuracies['Text Sequence']}\")\n",
        "        print(f\"Feature Model Accuracies: {accuracies['Feature']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68VU9nNSElpb",
        "outputId": "644ea60f-156f-4dc9-d441-655c74cc80c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 20% of the data...\n",
            "Results with 20% of the data:\n",
            "Emoticon Model Accuracies: {'Logistic Regression': 0.5153374233128835, 'SVM': 0.5153374233128835, 'Decision Tree': 0.5153374233128835}\n",
            "Text Sequence Model Accuracies: {'Logistic Regression': 0.5173824130879345, 'SVM': 0.49897750511247446, 'Decision Tree': 0.49284253578732107}\n",
            "Feature Model Accuracies: {'Logistic Regression': 0.9529652351738241, 'SVM': 0.967280163599182, 'Decision Tree': 0.9059304703476483}\n",
            "Training with 40% of the data...\n",
            "Results with 40% of the data:\n",
            "Emoticon Model Accuracies: {'Logistic Regression': 0.48466257668711654, 'SVM': 0.48466257668711654, 'Decision Tree': 0.48466257668711654}\n",
            "Text Sequence Model Accuracies: {'Logistic Regression': 0.5030674846625767, 'SVM': 0.5071574642126789, 'Decision Tree': 0.50920245398773}\n",
            "Feature Model Accuracies: {'Logistic Regression': 0.9652351738241309, 'SVM': 0.9754601226993865, 'Decision Tree': 0.9406952965235174}\n",
            "Training with 60% of the data...\n",
            "Results with 60% of the data:\n",
            "Emoticon Model Accuracies: {'Logistic Regression': 0.5153374233128835, 'SVM': 0.5153374233128835, 'Decision Tree': 0.5153374233128835}\n",
            "Text Sequence Model Accuracies: {'Logistic Regression': 0.5153374233128835, 'SVM': 0.5030674846625767, 'Decision Tree': 0.50920245398773}\n",
            "Feature Model Accuracies: {'Logistic Regression': 0.9795501022494888, 'SVM': 0.9795501022494888, 'Decision Tree': 0.9468302658486708}\n",
            "Training with 80% of the data...\n",
            "Results with 80% of the data:\n",
            "Emoticon Model Accuracies: {'Logistic Regression': 0.5153374233128835, 'SVM': 0.5153374233128835, 'Decision Tree': 0.5153374233128835}\n",
            "Text Sequence Model Accuracies: {'Logistic Regression': 0.5153374233128835, 'SVM': 0.5276073619631901, 'Decision Tree': 0.5255623721881391}\n",
            "Feature Model Accuracies: {'Logistic Regression': 0.9877300613496932, 'SVM': 0.9897750511247444, 'Decision Tree': 0.9591002044989775}\n",
            "Training with 100% of the data...\n",
            "Results with 100% of the data:\n",
            "Emoticon Model Accuracies: {'Logistic Regression': 0.5153374233128835, 'SVM': 0.5153374233128835, 'Decision Tree': 0.5153374233128835}\n",
            "Text Sequence Model Accuracies: {'Logistic Regression': 0.5153374233128835, 'SVM': 0.5255623721881391, 'Decision Tree': 0.5398773006134969}\n",
            "Feature Model Accuracies: {'Logistic Regression': 0.9815950920245399, 'SVM': 0.9877300613496932, 'Decision Tree': 0.9631901840490797}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below is LTSM neural net with utf-8 encoding for emoticon_dataset, very slow**"
      ],
      "metadata": {
        "id": "nnqGiUtgp_Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert emoji string into a list of Unicode (UTF-8) encoded integers\n",
        "def utf8_encode_emoticon_data(emoticon_data, max_length=13):\n",
        "    encoded_data = []\n",
        "    for sequence in emoticon_data:\n",
        "        utf8_encoded_sequence = [ord(emoji) for emoji in sequence]  # Convert each emoji to its Unicode code point\n",
        "        encoded_data.append(utf8_encoded_sequence)\n",
        "    padded_data = pad_sequences(encoded_data, maxlen=max_length, padding='post')\n",
        "    return padded_data\n",
        "\n",
        "def get_data_split(X, Y, split_ratio):\n",
        "    if split_ratio >= 1.0:\n",
        "        train_X, _, train_Y, _ = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
        "        return train_X, train_Y\n",
        "    train_X, _, train_Y, _ = train_test_split(X, Y, test_size=1 - split_ratio, random_state=42)\n",
        "    return train_X, train_Y\n",
        "\n",
        "def build_lstm_model(max_length, vocab_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))  # Embedding Layer\n",
        "    model.add(SpatialDropout1D(0.2))  # Dropout Layer to prevent overfitting\n",
        "    model.add(LSTM(100))  # LSTM Layer\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # Compile model\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate(model, train_X, train_Y, val_X, val_Y):\n",
        "    print(f\"train_X shape: {train_X.shape}, val_X shape: {val_X.shape}\")\n",
        "\n",
        "    train_Y = np.array(train_Y)\n",
        "    model.fit(train_X, train_Y, epochs=5, batch_size=32, verbose=1)  # Train for a few epochs\n",
        "\n",
        "    # Predictions\n",
        "    predictions = (model.predict(val_X) > 0.5).astype(\"int32\")\n",
        "    accuracy = accuracy_score(val_Y, predictions)\n",
        "    return accuracy\n",
        "\n",
        "def check_for_invalid_values(data):\n",
        "    if not all(isinstance(item, str) for item in data):\n",
        "        print(\"Data contains non-string values.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load your data here. Replace with your actual loading function.\n",
        "    (train_emoticon_X, train_emoticon_Y), (train_seq_X, train_seq_Y), (train_feat_X, train_feat_Y), \\\n",
        "    (val_emoticon_X, val_emoticon_Y), (val_seq_X, val_seq_Y), (val_feat_X, val_feat_Y) = load_data()\n",
        "\n",
        "    check_for_invalid_values(train_emoticon_X)\n",
        "    check_for_invalid_values(val_emoticon_X)\n",
        "\n",
        "    splits = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "    max_length = 13  # Adjust based on your data\n",
        "\n",
        "    for split in splits:\n",
        "        print(f\"\\nTraining with {int(split * 100)}% of the data...\\n\")\n",
        "\n",
        "        train_emoticon_X_split, train_emoticon_Y_split = get_data_split(train_emoticon_X, train_emoticon_Y, split)\n",
        "\n",
        "        # Preprocess emoji data by encoding them using UTF-8\n",
        "        train_emoticon_X_encoded = utf8_encode_emoticon_data(train_emoticon_X_split, max_length=max_length)\n",
        "        val_emoticon_X_encoded = utf8_encode_emoticon_data(val_emoticon_X, max_length=max_length)\n",
        "\n",
        "        # Vocabulary size is based on the Unicode range (max value + 1)\n",
        "        vocab_size = max([max(seq) for seq in train_emoticon_X_encoded]) + 1\n",
        "\n",
        "        # Build the LSTM model\n",
        "        model = build_lstm_model(max_length, vocab_size)\n",
        "\n",
        "        # Train the model and evaluate accuracy\n",
        "        accuracy = train_and_evaluate(model, train_emoticon_X_encoded, train_emoticon_Y_split,\n",
        "                                      val_emoticon_X_encoded, val_emoticon_Y)\n",
        "        print(f\"Accuracy with {int(split * 100)}% of the data: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYFKsNjviAJj",
        "outputId": "30877fcb-e476-4f26-8f3e-866d866c704c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with 20% of the data...\n",
            "\n",
            "train_X shape: (1416, 13), val_X shape: (489, 13)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 291ms/step - accuracy: 0.5218 - loss: 0.6924\n",
            "Epoch 2/5\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 291ms/step - accuracy: 0.7254 - loss: 0.6440\n",
            "Epoch 3/5\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 267ms/step - accuracy: 0.8568 - loss: 0.3881\n",
            "Epoch 4/5\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 303ms/step - accuracy: 0.9012 - loss: 0.2719\n",
            "Epoch 5/5\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 276ms/step - accuracy: 0.9050 - loss: 0.2508\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Accuracy with 20% of the data: 0.87\n",
            "\n",
            "Training with 40% of the data...\n",
            "\n",
            "train_X shape: (2832, 13), val_X shape: (489, 13)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 282ms/step - accuracy: 0.5391 - loss: 0.6864\n",
            "Epoch 2/5\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 295ms/step - accuracy: 0.8286 - loss: 0.3850\n",
            "Epoch 3/5\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 285ms/step - accuracy: 0.8969 - loss: 0.2576\n",
            "Epoch 4/5\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 277ms/step - accuracy: 0.9014 - loss: 0.2328\n",
            "Epoch 5/5\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 275ms/step - accuracy: 0.9206 - loss: 0.1939\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "Accuracy with 40% of the data: 0.90\n",
            "\n",
            "Training with 60% of the data...\n",
            "\n",
            "train_X shape: (4248, 13), val_X shape: (489, 13)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 278ms/step - accuracy: 0.5594 - loss: 0.6695\n",
            "Epoch 2/5\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 314ms/step - accuracy: 0.8629 - loss: 0.3302\n",
            "Epoch 3/5\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 277ms/step - accuracy: 0.9138 - loss: 0.2100\n",
            "Epoch 4/5\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 284ms/step - accuracy: 0.9303 - loss: 0.1732\n",
            "Epoch 5/5\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 277ms/step - accuracy: 0.9390 - loss: 0.1441\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Accuracy with 60% of the data: 0.90\n",
            "\n",
            "Training with 80% of the data...\n",
            "\n",
            "train_X shape: (5664, 13), val_X shape: (489, 13)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 275ms/step - accuracy: 0.6107 - loss: 0.6268\n",
            "Epoch 2/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 296ms/step - accuracy: 0.8835 - loss: 0.2799\n",
            "Epoch 3/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 279ms/step - accuracy: 0.9193 - loss: 0.1881\n",
            "Epoch 4/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 279ms/step - accuracy: 0.9377 - loss: 0.1394\n",
            "Epoch 5/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 278ms/step - accuracy: 0.9570 - loss: 0.1093\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Accuracy with 80% of the data: 0.91\n",
            "\n",
            "Training with 100% of the data...\n",
            "\n",
            "train_X shape: (6372, 13), val_X shape: (489, 13)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 289ms/step - accuracy: 0.6077 - loss: 0.6149\n",
            "Epoch 2/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 278ms/step - accuracy: 0.8966 - loss: 0.2466\n",
            "Epoch 3/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 293ms/step - accuracy: 0.9313 - loss: 0.1702\n",
            "Epoch 4/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 281ms/step - accuracy: 0.9418 - loss: 0.1342\n",
            "Epoch 5/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 284ms/step - accuracy: 0.9526 - loss: 0.1123\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Accuracy with 100% of the data: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below is LTSM neural net with utf-8 then one hot encoding for emoticon_dataset, very fast**"
      ],
      "metadata": {
        "id": "aqWZ5ZuKqD4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Convert emoji string into a list of Unicode (UTF-8) encoded integers\n",
        "def utf8_encode_emoticon_data(emoticon_data, max_length=13):\n",
        "    encoded_data = []\n",
        "    all_unique_emojis = set()\n",
        "\n",
        "    for sequence in emoticon_data:\n",
        "        utf8_encoded_sequence = [ord(emoji) for emoji in sequence]  # Convert each emoji to its Unicode code point\n",
        "        encoded_data.append(utf8_encoded_sequence)\n",
        "        all_unique_emojis.update(utf8_encoded_sequence)\n",
        "\n",
        "    # Padding sequences to the same length\n",
        "    padded_data = pad_sequences(encoded_data, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Return the padded sequences and the unique set of Unicode code points\n",
        "    return padded_data, sorted(list(all_unique_emojis))\n",
        "\n",
        "# Create one-hot encoding for sequences based on unique Unicode values\n",
        "def one_hot_encode_sequences(padded_data, unique_emojis, max_length):\n",
        "    vocab_size = len(unique_emojis)\n",
        "    emoji_to_index = {emoji: idx for idx, emoji in enumerate(unique_emojis)}\n",
        "\n",
        "    one_hot_encoded_data = np.zeros((len(padded_data), max_length, vocab_size), dtype='float32')\n",
        "\n",
        "    for i, sequence in enumerate(padded_data):\n",
        "        for j, emoji in enumerate(sequence):\n",
        "            if emoji in emoji_to_index:\n",
        "                one_hot_encoded_data[i, j, emoji_to_index[emoji]] = 1.0\n",
        "\n",
        "    return one_hot_encoded_data\n",
        "\n",
        "# Filter out sequences containing the missing data emoji (ğŸ›“) and filter the corresponding labels\n",
        "def filter_sequences(emoticon_data, labels, placeholder_emoji='ğŸ›“'):\n",
        "    filtered_sequences = []\n",
        "    filtered_labels = []\n",
        "\n",
        "    for i, sequence in enumerate(emoticon_data):\n",
        "        if placeholder_emoji not in sequence:\n",
        "            filtered_sequences.append(sequence)\n",
        "            filtered_labels.append(labels[i])\n",
        "\n",
        "    return filtered_sequences, filtered_labels\n",
        "\n",
        "def get_data_split(X, Y, split_ratio):\n",
        "    if split_ratio >= 1.0:\n",
        "        train_X, _, train_Y, _ = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
        "        return train_X, train_Y\n",
        "    train_X, _, train_Y, _ = train_test_split(X, Y, test_size=1 - split_ratio, random_state=42)\n",
        "    return train_X, train_Y\n",
        "\n",
        "def build_lstm_model(max_length, vocab_size):\n",
        "    model = Sequential()\n",
        "    model.add(SpatialDropout1D(0.2))  # Dropout Layer to prevent overfitting\n",
        "    model.add(LSTM(100, input_shape=(max_length, vocab_size)))  # LSTM Layer with input_shape defined here\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # Compile model\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate(model, train_X, train_Y, val_X, val_Y):\n",
        "    print(f\"train_X shape: {train_X.shape}, val_X shape: {val_X.shape}\")\n",
        "\n",
        "    train_Y = np.array(train_Y)\n",
        "    model.fit(train_X, train_Y, epochs=5, batch_size=32, verbose=1)  # Train for a few epochs\n",
        "\n",
        "    # Predictions\n",
        "    predictions = (model.predict(val_X) > 0.5).astype(\"int32\")\n",
        "    accuracy = accuracy_score(val_Y, predictions)\n",
        "    return accuracy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Assuming load_data function is already defined and loads the dataset\n",
        "    (train_emoticon_X, train_emoticon_Y), (train_seq_X, train_seq_Y), (train_feat_X, train_feat_Y), \\\n",
        "    (val_emoticon_X, val_emoticon_Y), (val_seq_X, val_seq_Y), (val_feat_X, val_feat_Y) = load_data()\n",
        "\n",
        "    # Filter out sequences that contain the ğŸ›“ placeholder emoji along with corresponding labels\n",
        "    train_emoticon_X_filtered, train_emoticon_Y_filtered = filter_sequences(train_emoticon_X, train_emoticon_Y, placeholder_emoji='ğŸ›“')\n",
        "    val_emoticon_X_filtered, val_emoticon_Y_filtered = filter_sequences(val_emoticon_X, val_emoticon_Y, placeholder_emoji='ğŸ›“')\n",
        "\n",
        "    splits = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "    max_length = 13  # Adjust based on your data\n",
        "\n",
        "    for split in splits:\n",
        "        print(f\"\\nTraining with {int(split * 100)}% of the data...\\n\")\n",
        "\n",
        "        # Get the split of the filtered training data\n",
        "        train_emoticon_X_split, train_emoticon_Y_split = get_data_split(train_emoticon_X_filtered, train_emoticon_Y_filtered, split)\n",
        "\n",
        "        # Preprocess emoji data by encoding them using UTF-8\n",
        "        train_emoticon_X_encoded, unique_emojis = utf8_encode_emoticon_data(train_emoticon_X_split, max_length=max_length)\n",
        "        val_emoticon_X_encoded, _ = utf8_encode_emoticon_data(val_emoticon_X_filtered, max_length=max_length)\n",
        "\n",
        "        # One-hot encode the emoji sequences\n",
        "        train_emoticon_X_one_hot = one_hot_encode_sequences(train_emoticon_X_encoded, unique_emojis, max_length)\n",
        "        val_emoticon_X_one_hot = one_hot_encode_sequences(val_emoticon_X_encoded, unique_emojis, max_length)\n",
        "\n",
        "        # Build the LSTM model without an embedding layer (since we're using one-hot encoding)\n",
        "        vocab_size = len(unique_emojis)\n",
        "        model = build_lstm_model(max_length, vocab_size)\n",
        "\n",
        "        # Train the model and evaluate accuracy\n",
        "        accuracy = train_and_evaluate(model, train_emoticon_X_one_hot, train_emoticon_Y_split,\n",
        "                                      val_emoticon_X_one_hot, val_emoticon_Y_filtered)\n",
        "        print(f\"Accuracy with {int(split * 100)}% of the data: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWNco8dBkNQn",
        "outputId": "ec7b6e34-a229-46ac-e852-e10d17ba85b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with 20% of the data...\n",
            "\n",
            "train_X shape: (1384, 13, 213), val_X shape: (475, 13, 213)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.5152 - loss: 0.6925\n",
            "Epoch 2/5\n",
            "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5959 - loss: 0.6821\n",
            "Epoch 3/5\n",
            "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6926 - loss: 0.6024\n",
            "Epoch 4/5\n",
            "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7538 - loss: 0.4930\n",
            "Epoch 5/5\n",
            "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8017 - loss: 0.4418\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Accuracy with 20% of the data: 0.77\n",
            "\n",
            "Training with 40% of the data...\n",
            "\n",
            "train_X shape: (2768, 13, 213), val_X shape: (475, 13, 213)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.5017 - loss: 0.6921\n",
            "Epoch 2/5\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6954 - loss: 0.6275\n",
            "Epoch 3/5\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.7764 - loss: 0.4846\n",
            "Epoch 4/5\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8201 - loss: 0.3894\n",
            "Epoch 5/5\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8416 - loss: 0.3672\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
            "Accuracy with 40% of the data: 0.87\n",
            "\n",
            "Training with 60% of the data...\n",
            "\n",
            "train_X shape: (4152, 13, 213), val_X shape: (475, 13, 213)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m130/130\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.5412 - loss: 0.6870\n",
            "Epoch 2/5\n",
            "\u001b[1m130/130\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.7249 - loss: 0.5304\n",
            "Epoch 3/5\n",
            "\u001b[1m130/130\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.7970 - loss: 0.4296\n",
            "Epoch 4/5\n",
            "\u001b[1m130/130\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8273 - loss: 0.3739\n",
            "Epoch 5/5\n",
            "\u001b[1m130/130\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - accuracy: 0.8525 - loss: 0.3452\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Accuracy with 60% of the data: 0.92\n",
            "\n",
            "Training with 80% of the data...\n",
            "\n",
            "train_X shape: (5536, 13, 213), val_X shape: (475, 13, 213)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m173/173\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.5412 - loss: 0.6809\n",
            "Epoch 2/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.7671 - loss: 0.4970\n",
            "Epoch 3/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.8207 - loss: 0.3948\n",
            "Epoch 4/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.8381 - loss: 0.3726\n",
            "Epoch 5/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.8396 - loss: 0.3500\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
            "Accuracy with 80% of the data: 0.92\n",
            "\n",
            "Training with 100% of the data...\n",
            "\n",
            "train_X shape: (6228, 13, 213), val_X shape: (475, 13, 213)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m195/195\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.5734 - loss: 0.6755\n",
            "Epoch 2/5\n",
            "\u001b[1m195/195\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7922 - loss: 0.4656\n",
            "Epoch 3/5\n",
            "\u001b[1m195/195\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8277 - loss: 0.3695\n",
            "Epoch 4/5\n",
            "\u001b[1m195/195\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.8521 - loss: 0.3448\n",
            "Epoch 5/5\n",
            "\u001b[1m195/195\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.8531 - loss: 0.3277\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Accuracy with 100% of the data: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now to improve on the text sequence dataset.**"
      ],
      "metadata": {
        "id": "2WBrh7dZK4I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, SpatialDropout1D, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Encode sequence data\n",
        "def encode_sequence_data(sequence_data, max_length=50):\n",
        "    encoded_data = []\n",
        "    all_unique_numbers = set()\n",
        "\n",
        "    for sequence in sequence_data:\n",
        "        if isinstance(sequence, (str, int)):\n",
        "            sequence_str = str(sequence)\n",
        "            encoded_sequence = [int(char) for char in sequence_str if char.isdigit()]\n",
        "            encoded_data.append(encoded_sequence)\n",
        "            all_unique_numbers.update(encoded_sequence)\n",
        "        else:\n",
        "            print(f\"Skipping invalid sequence: {sequence}\")\n",
        "\n",
        "    padded_data = pad_sequences(encoded_data, maxlen=max_length, padding='post')\n",
        "\n",
        "    return padded_data, len(all_unique_numbers)\n",
        "\n",
        "# Build the RNN model with LSTM or GRU\n",
        "def build_rnn_model(input_length, vocab_size, use_gru=False):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size + 1, output_dim=128, input_length=input_length))\n",
        "    model.add(SpatialDropout1D(0.3))  # Dropout after embedding layer\n",
        "\n",
        "    if use_gru:\n",
        "        model.add(GRU(128, return_sequences=True))\n",
        "        model.add(GRU(128))\n",
        "    else:\n",
        "        model.add(LSTM(128, return_sequences=True))\n",
        "        model.add(LSTM(128))\n",
        "\n",
        "    model.add(Dropout(0.3))  # Dropout after LSTM/GRU layers\n",
        "    model.add(Dense(64, activation='relu'))  # Dense layer before output\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train and evaluate the model\n",
        "def train_and_evaluate(model, train_X, train_Y, val_X, val_Y):\n",
        "    print(f\"train_X shape: {train_X.shape}, val_X shape: {val_X.shape}\")\n",
        "\n",
        "    train_Y = np.array(train_Y)\n",
        "    val_Y = np.array(val_Y)\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_X, train_Y, epochs=20, batch_size=32, validation_data=(val_X, val_Y),\n",
        "              callbacks=[early_stopping, lr_scheduler], verbose=1)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    predictions = (model.predict(val_X) > 0.5).astype(\"int32\")\n",
        "    accuracy = accuracy_score(val_Y, predictions)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Main script\n",
        "if __name__ == '__main__':\n",
        "    # Load the datasets\n",
        "    (train_emoticon_X, train_emoticon_Y), (train_seq_X, train_seq_Y), (train_feat_X, train_feat_Y), \\\n",
        "    (val_emoticon_X, val_emoticon_Y), (val_seq_X, val_seq_Y), (val_feat_X, val_feat_Y) = load_data()\n",
        "\n",
        "    # Encode the sequence data\n",
        "    max_length = 50\n",
        "    train_seq_X_encoded, vocab_size = encode_sequence_data(train_seq_X, max_length=max_length)\n",
        "    val_seq_X_encoded, _ = encode_sequence_data(val_seq_X, max_length=max_length)\n",
        "\n",
        "    # Build and compile the model\n",
        "    model = build_rnn_model(input_length=max_length, vocab_size=vocab_size)\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    accuracy = train_and_evaluate(model, train_seq_X_encoded, train_seq_Y,\n",
        "                                  val_seq_X_encoded, val_seq_Y)\n",
        "\n",
        "    print(f\"Validation Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsFbmqhyLAoe",
        "outputId": "b046d1e1-f962-463a-92e5-d17872fd9a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X shape: (7080, 50), val_X shape: (489, 50)\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 223ms/step - accuracy: 0.5152 - loss: 0.6905 - val_accuracy: 0.6094 - val_loss: 0.6456 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 214ms/step - accuracy: 0.6206 - loss: 0.6482 - val_accuracy: 0.6524 - val_loss: 0.6366 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 209ms/step - accuracy: 0.6188 - loss: 0.6458 - val_accuracy: 0.6503 - val_loss: 0.6190 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 214ms/step - accuracy: 0.6379 - loss: 0.6369 - val_accuracy: 0.6708 - val_loss: 0.6149 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 214ms/step - accuracy: 0.6398 - loss: 0.6229 - val_accuracy: 0.6789 - val_loss: 0.6101 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 213ms/step - accuracy: 0.6584 - loss: 0.6198 - val_accuracy: 0.6830 - val_loss: 0.6169 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 230ms/step - accuracy: 0.6756 - loss: 0.5958 - val_accuracy: 0.6953 - val_loss: 0.5894 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 216ms/step - accuracy: 0.6949 - loss: 0.5763 - val_accuracy: 0.6830 - val_loss: 0.5885 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 213ms/step - accuracy: 0.7028 - loss: 0.5620 - val_accuracy: 0.7260 - val_loss: 0.5467 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 229ms/step - accuracy: 0.7321 - loss: 0.5279 - val_accuracy: 0.7382 - val_loss: 0.5250 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 220ms/step - accuracy: 0.7666 - loss: 0.4779 - val_accuracy: 0.7403 - val_loss: 0.4994 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 222ms/step - accuracy: 0.7952 - loss: 0.4211 - val_accuracy: 0.7873 - val_loss: 0.4745 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 269ms/step - accuracy: 0.8320 - loss: 0.3678 - val_accuracy: 0.7996 - val_loss: 0.4040 - learning_rate: 0.0010\n",
            "Epoch 14/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 227ms/step - accuracy: 0.8496 - loss: 0.3369 - val_accuracy: 0.8344 - val_loss: 0.3714 - learning_rate: 0.0010\n",
            "Epoch 15/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 220ms/step - accuracy: 0.8770 - loss: 0.2871 - val_accuracy: 0.8671 - val_loss: 0.3117 - learning_rate: 0.0010\n",
            "Epoch 16/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 216ms/step - accuracy: 0.8961 - loss: 0.2491 - val_accuracy: 0.8691 - val_loss: 0.3177 - learning_rate: 0.0010\n",
            "Epoch 17/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 225ms/step - accuracy: 0.9055 - loss: 0.2217 - val_accuracy: 0.8732 - val_loss: 0.3377 - learning_rate: 0.0010\n",
            "Epoch 18/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 221ms/step - accuracy: 0.9256 - loss: 0.1828 - val_accuracy: 0.8875 - val_loss: 0.2926 - learning_rate: 0.0010\n",
            "Epoch 19/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 210ms/step - accuracy: 0.9301 - loss: 0.1710 - val_accuracy: 0.8916 - val_loss: 0.3250 - learning_rate: 0.0010\n",
            "Epoch 20/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 215ms/step - accuracy: 0.9438 - loss: 0.1438 - val_accuracy: 0.8732 - val_loss: 0.3605 - learning_rate: 0.0010\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 153ms/step\n",
            "Validation Accuracy: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2**"
      ],
      "metadata": {
        "id": "Y7AOo2FZo6vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define your functions to encode the data\n",
        "def utf8_encode_emoticon_data(emoticon_data, max_length=13):\n",
        "    encoded_data = []\n",
        "    all_unique_emojis = set()\n",
        "\n",
        "    for sequence in emoticon_data:\n",
        "        utf8_encoded_sequence = [ord(emoji) for emoji in sequence]\n",
        "        encoded_data.append(utf8_encoded_sequence)\n",
        "        all_unique_emojis.update(utf8_encoded_sequence)\n",
        "\n",
        "    padded_data = pad_sequences(encoded_data, maxlen=max_length, padding='post')\n",
        "    return padded_data, sorted(list(all_unique_emojis))\n",
        "\n",
        "def one_hot_encode_sequences(padded_data, unique_emojis, max_length):\n",
        "    vocab_size = len(unique_emojis)\n",
        "    emoji_to_index = {emoji: idx for idx, emoji in enumerate(unique_emojis)}\n",
        "\n",
        "    one_hot_encoded_data = np.zeros((len(padded_data), max_length, vocab_size), dtype='float32')\n",
        "\n",
        "    for i, sequence in enumerate(padded_data):\n",
        "        for j, emoji in enumerate(sequence):\n",
        "            if emoji in emoji_to_index:\n",
        "                one_hot_encoded_data[i, j, emoji_to_index[emoji]] = 1.0\n",
        "\n",
        "    return one_hot_encoded_data\n",
        "\n",
        "\n",
        "def encode_sequence_data(sequence_data, max_length=50):\n",
        "    encoded_data = []\n",
        "    all_unique_numbers = set()\n",
        "\n",
        "    for sequence in sequence_data:\n",
        "        if isinstance(sequence, (str, int)):\n",
        "            sequence_str = str(sequence)\n",
        "            encoded_sequence = [int(char) for char in sequence_str if char.isdigit()]\n",
        "            encoded_data.append(encoded_sequence)\n",
        "            all_unique_numbers.update(encoded_sequence)\n",
        "        else:\n",
        "            print(f\"Skipping invalid sequence: {sequence}\")\n",
        "\n",
        "    padded_data = pad_sequences(encoded_data, maxlen=max_length, padding='post')\n",
        "    return padded_data, len(all_unique_numbers)\n",
        "\n",
        "# Adjust the emoticon vocab size after encoding\n",
        "def map_emojis_to_indices(data, emoji_to_index):\n",
        "    mapped_data = []\n",
        "    for sequence in data:\n",
        "        mapped_sequence = [emoji_to_index.get(emoji, 0) for emoji in sequence]\n",
        "        mapped_data.append(mapped_sequence)\n",
        "    return np.array(mapped_data)\n",
        "\n",
        "# Build the combined model\n",
        "def build_combined_model(emoticon_vocab_size, emoticon_input_len, text_seq_input_len, feature_input_shape):\n",
        "    # Input 1: Emoticon input\n",
        "    emoticon_input = Input(shape=(emoticon_input_len,))\n",
        "    x1 = Embedding(input_dim=emoticon_vocab_size, output_dim=128)(emoticon_input)\n",
        "    x1 = SpatialDropout1D(0.2)(x1)\n",
        "    x1 = LSTM(64)(x1)\n",
        "\n",
        "    # Input 2: Text sequence input\n",
        "    text_input = Input(shape=(text_seq_input_len,))\n",
        "    x2 = Embedding(10000, 128)(text_input)\n",
        "    x2 = SpatialDropout1D(0.2)(x2)\n",
        "    x2 = LSTM(64)(x2)\n",
        "\n",
        "    # Input 3: Feature input\n",
        "    feature_input = Input(shape=feature_input_shape)\n",
        "    x3 = Dense(64, activation='relu')(feature_input)\n",
        "\n",
        "    # Concatenate all inputs\n",
        "    concatenated = Concatenate()([x1, x2, x3])\n",
        "\n",
        "    # Dense layers after concatenation\n",
        "    x = Dense(64, activation='relu')(concatenated)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[emoticon_input, text_input, feature_input], outputs=output)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Assume load_data() is defined and loads the datasets\n",
        "(train_emoticon_X, train_emoticon_Y), (train_seq_X, train_seq_Y), (train_feat_X, train_feat_Y), \\\n",
        "(val_emoticon_X, val_emoticon_Y), (val_seq_X, val_seq_Y), (val_feat_X, val_feat_Y) = load_data()\n",
        "\n",
        "# Encode emoticon data\n",
        "train_emoticon_X, unique_emojis = utf8_encode_emoticon_data(train_emoticon_X)\n",
        "val_emoticon_X, val_unique_emojis = utf8_encode_emoticon_data(val_emoticon_X)\n",
        "unique_emojis = sorted(list(set(unique_emojis + val_unique_emojis)))\n",
        "emoji_to_index = {emoji: idx for idx, emoji in enumerate(unique_emojis)}\n",
        "train_emoticon_X = map_emojis_to_indices(train_emoticon_X, emoji_to_index)\n",
        "val_emoticon_X = map_emojis_to_indices(val_emoticon_X, emoji_to_index)\n",
        "\n",
        "# Encode sequence data\n",
        "train_seq_X, _ = encode_sequence_data(train_seq_X)\n",
        "val_seq_X, _ = encode_sequence_data(val_seq_X)\n",
        "\n",
        "# Ensure feature input is correctly shaped\n",
        "train_feat_X = np.array(train_feat_X)  # Shape (7080, 13, 768)\n",
        "train_feat_X = train_feat_X.reshape(-1, 13 * 768)  # Flatten feature input for compatibility\n",
        "val_feat_X = np.array(val_feat_X).reshape(-1, 13 * 768)\n",
        "\n",
        "# Hyperparameters\n",
        "emoticon_vocab_size = len(unique_emojis)  # Use the actual vocab size from unique emojis\n",
        "emoticon_input_len = train_emoticon_X.shape[1]  # Shape (7080, 13)\n",
        "text_seq_input_len = train_seq_X.shape[1]  # Shape (7080, 50)\n",
        "feature_input_shape = (train_feat_X.shape[1],)  # Now (13 * 768,)\n",
        "\n",
        "# Build the combined model\n",
        "combined_model = build_combined_model(emoticon_vocab_size, emoticon_input_len, text_seq_input_len, feature_input_shape)\n",
        "\n",
        "# Add data splits and accuracies tracking\n",
        "data_splits = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "accuracies = []\n",
        "\n",
        "# Loop over the data splits\n",
        "for split in data_splits:\n",
        "    # Calculate number of samples for this split\n",
        "    split_size = int(len(train_emoticon_X) * split)\n",
        "\n",
        "    # Prepare the training data by slicing according to split size\n",
        "    train_emoticon_X_split = train_emoticon_X[:split_size]\n",
        "    train_emoticon_Y_split = np.array(train_emoticon_Y[:split_size])\n",
        "\n",
        "    train_seq_X_split = train_seq_X[:split_size]\n",
        "    train_seq_Y_split = np.array(train_seq_Y[:split_size])\n",
        "\n",
        "    train_feat_X_split = train_feat_X[:split_size]\n",
        "    train_feat_Y_split = np.array(train_feat_Y[:split_size])\n",
        "    val_feat_Y_split = np.array(val_feat_Y[:split_size])\n",
        "\n",
        "    # Fit the combined model with the split data\n",
        "    combined_model.fit(\n",
        "        [train_emoticon_X_split, train_seq_X_split, train_feat_X_split],\n",
        "        train_emoticon_Y_split,\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the validation sets\n",
        "    accuracy = combined_model.evaluate(\n",
        "    [val_emoticon_X, val_seq_X, val_feat_X],\n",
        "    val_feat_Y,  # Assuming your validation labels are the same for all datasets\n",
        "    verbose=1\n",
        "    )[1]  # [1] gets accuracy\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Print the result for this split\n",
        "    print(f\"Accuracy for {int(split * 100)}% of the data: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "0uMQ99X9sbiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba58ca0-56fb-4169-bf2a-855a7ecf83a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.6731 - loss: 0.6121\n",
            "Epoch 2/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 69ms/step - accuracy: 0.9113 - loss: 0.2288\n",
            "Epoch 3/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 94ms/step - accuracy: 0.9497 - loss: 0.1266\n",
            "Epoch 4/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - accuracy: 0.9336 - loss: 0.1513\n",
            "Epoch 5/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - accuracy: 0.9778 - loss: 0.0786\n",
            "Epoch 6/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 104ms/step - accuracy: 0.9751 - loss: 0.0599\n",
            "Epoch 7/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.9936 - loss: 0.0297\n",
            "Epoch 8/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9526 - loss: 0.1110\n",
            "Epoch 9/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.9597 - loss: 0.0885\n",
            "Epoch 10/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 113ms/step - accuracy: 0.9966 - loss: 0.0198\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9472 - loss: 0.1479\n",
            "Accuracy for 20% of the data: 0.9447852969169617\n",
            "Epoch 1/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - accuracy: 0.9473 - loss: 0.1318\n",
            "Epoch 2/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 110ms/step - accuracy: 0.9767 - loss: 0.0617\n",
            "Epoch 3/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 106ms/step - accuracy: 0.9623 - loss: 0.0970\n",
            "Epoch 4/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 92ms/step - accuracy: 0.9875 - loss: 0.0365\n",
            "Epoch 5/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 95ms/step - accuracy: 0.9691 - loss: 0.0768\n",
            "Epoch 6/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 79ms/step - accuracy: 0.9754 - loss: 0.0643\n",
            "Epoch 7/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 82ms/step - accuracy: 0.9941 - loss: 0.0215\n",
            "Epoch 8/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 72ms/step - accuracy: 0.9925 - loss: 0.0258\n",
            "Epoch 9/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 89ms/step - accuracy: 0.9968 - loss: 0.0142\n",
            "Epoch 10/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - accuracy: 0.9866 - loss: 0.0340\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9535 - loss: 0.1481\n",
            "Accuracy for 40% of the data: 0.9488752484321594\n",
            "Epoch 1/10\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - accuracy: 0.9543 - loss: 0.1358\n",
            "Epoch 2/10\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 84ms/step - accuracy: 0.9809 - loss: 0.0474\n",
            "Epoch 3/10\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 81ms/step - accuracy: 0.9928 - loss: 0.0234\n",
            "Epoch 4/10\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 93ms/step - accuracy: 0.9807 - loss: 0.0473\n",
            "Epoch 5/10\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 70ms/step - accuracy: 0.9852 - loss: 0.0387\n",
            "Epoch 6/10\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - accuracy: 0.9855 - loss: 0.0450\n",
            "Epoch 7/10\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 89ms/step - accuracy: 0.9977 - loss: 0.0107\n",
            "Epoch 8/10\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 84ms/step - accuracy: 0.9844 - loss: 0.0399\n",
            "Epoch 9/10\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 88ms/step - accuracy: 0.9908 - loss: 0.0249\n",
            "Epoch 10/10\n",
            "\u001b[1m133/133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 82ms/step - accuracy: 0.9891 - loss: 0.0245\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9819 - loss: 0.0572\n",
            "Accuracy for 60% of the data: 0.977505087852478\n",
            "Epoch 1/10\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 97ms/step - accuracy: 0.9846 - loss: 0.0415\n",
            "Epoch 2/10\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 84ms/step - accuracy: 0.9837 - loss: 0.0414\n",
            "Epoch 3/10\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 81ms/step - accuracy: 0.9848 - loss: 0.0422\n",
            "Epoch 4/10\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 83ms/step - accuracy: 0.9890 - loss: 0.0329\n",
            "Epoch 5/10\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 86ms/step - accuracy: 0.9898 - loss: 0.0279\n",
            "Epoch 6/10\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 80ms/step - accuracy: 0.9920 - loss: 0.0296\n",
            "Epoch 7/10\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 84ms/step - accuracy: 0.9883 - loss: 0.0331\n",
            "Epoch 8/10\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 85ms/step - accuracy: 0.9953 - loss: 0.0129\n",
            "Epoch 9/10\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 87ms/step - accuracy: 0.9928 - loss: 0.0196\n",
            "Epoch 10/10\n",
            "\u001b[1m177/177\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 85ms/step - accuracy: 0.9929 - loss: 0.0170\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9576 - loss: 0.1224\n",
            "Accuracy for 80% of the data: 0.9631901979446411\n",
            "Epoch 1/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 82ms/step - accuracy: 0.9821 - loss: 0.0496\n",
            "Epoch 2/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 82ms/step - accuracy: 0.9936 - loss: 0.0224\n",
            "Epoch 3/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 96ms/step - accuracy: 0.9846 - loss: 0.0428\n",
            "Epoch 4/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 80ms/step - accuracy: 0.9876 - loss: 0.0394\n",
            "Epoch 5/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 84ms/step - accuracy: 0.9884 - loss: 0.0286\n",
            "Epoch 6/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 79ms/step - accuracy: 0.9900 - loss: 0.0270\n",
            "Epoch 7/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 80ms/step - accuracy: 0.9909 - loss: 0.0252\n",
            "Epoch 8/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 80ms/step - accuracy: 0.9890 - loss: 0.0314\n",
            "Epoch 9/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 82ms/step - accuracy: 0.9946 - loss: 0.0145\n",
            "Epoch 10/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 81ms/step - accuracy: 0.9927 - loss: 0.0218\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9572 - loss: 0.1298\n",
            "Accuracy for 100% of the data: 0.9591001868247986\n"
          ]
        }
      ]
    }
  ]
}