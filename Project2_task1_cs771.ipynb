{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpPzEfMoXJmY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "4b78d40e-9ca2-47f4-d71f-3176fb19b3a3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9d7329fa2733>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfilterwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import accuracy_score\n",
        "from typing import List, Dict, Tuple\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "def mmd_loss(source_features: torch.Tensor, target_features: torch.Tensor,\n",
        "             kernel_type: str = 'rbf',\n",
        "             sigma: float = 1.0) -> torch.Tensor:\n",
        "    def gaussian_kernel(x, y, sigma=1.0):\n",
        "        x_size = x.size(0)\n",
        "        y_size = y.size(0)\n",
        "\n",
        "        # Compute squared pairwise distances\n",
        "        norm2 = torch.pow(x, 2).sum(1).unsqueeze(1) - 2 * torch.mm(x, y.t()) + torch.pow(y, 2).sum(1).unsqueeze(0)\n",
        "\n",
        "        # Compute kernel\n",
        "        return torch.exp(-norm2 / (2 * sigma * sigma))\n",
        "\n",
        "    def linear_kernel(x, y):\n",
        "        return torch.mm(x, y.t())\n",
        "\n",
        "    # Choose kernel\n",
        "    if kernel_type == 'rbf':\n",
        "        kernel = lambda x, y: gaussian_kernel(x, y, sigma)\n",
        "    else:\n",
        "        kernel = linear_kernel\n",
        "\n",
        "    # Compute kernel matrices\n",
        "    xx = kernel(source_features, source_features)\n",
        "    yy = kernel(target_features, target_features)\n",
        "    xy = kernel(source_features, target_features)\n",
        "\n",
        "    # Compute MMD\n",
        "    xx_mean = xx.mean()\n",
        "    yy_mean = yy.mean()\n",
        "    xy_mean = xy.mean()\n",
        "\n",
        "    return xx_mean + yy_mean - 2 * xy_mean\n",
        "\n",
        "def coral_loss(self, source, target):\n",
        "\n",
        "    d = source.size(1)\n",
        "\n",
        "    # Source covariance\n",
        "    source = source - torch.mean(source, dim=0, keepdim=True)\n",
        "    source_cov = torch.matmul(source.t(), source) / (source.size(0) - 1)\n",
        "\n",
        "\n",
        "    target = target - torch.mean(target, dim=0, keepdim=True)\n",
        "    target_cov = torch.matmul(target.t(), target) / (target.size(0) - 1)\n",
        "\n",
        "    # Frobenius norm between the two covariance matrices\n",
        "    loss = torch.norm(source_cov - target_cov, p='fro')\n",
        "    return loss / (4 * d * d)\n",
        "\n",
        "class ProbabilisticLwPClassifier:\n",
        "    def __init__(self, uncertainty_threshold=0.7, feature_dim=512):\n",
        "        self.class_prototypes = {}\n",
        "        self.class_uncertainties = {}\n",
        "        self.class_counts = {}\n",
        "        self.uncertainty_threshold = uncertainty_threshold\n",
        "        self.reg_matrix = torch.eye(feature_dim) * 1e-4\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, filepath):\n",
        "        drive_path = '/content/drive/MyDrive/ml_models/'\n",
        "        full_filepath = os.path.join(drive_path, filepath)\n",
        "\n",
        "        try:\n",
        "            model_state = torch.load(full_filepath)\n",
        "\n",
        "            loaded_model = cls(\n",
        "                uncertainty_threshold=model_state['uncertainty_threshold'],\n",
        "                feature_dim=model_state['feature_dim']\n",
        "            )\n",
        "\n",
        "            loaded_model.class_prototypes = {\n",
        "                k: torch.tensor(v) for k, v in model_state['class_prototypes'].items()\n",
        "            }\n",
        "\n",
        "            loaded_model.class_uncertainties = {\n",
        "                k: torch.tensor(v) for k, v in model_state['class_uncertainties'].items()\n",
        "            }\n",
        "\n",
        "            loaded_model.class_counts = model_state['class_counts']\n",
        "\n",
        "            print(f\"Model loaded successfully from {full_filepath}\")\n",
        "            return loaded_model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            return None\n",
        "\n",
        "    def train(self, train_data: torch.Tensor, train_labels: torch.Tensor,\n",
        "              prev_prototypes=None, prev_uncertainties=None, prev_counts=None):\n",
        "        unique_labels = torch.unique(train_labels)\n",
        "\n",
        "        for label in unique_labels:\n",
        "            mask = train_labels == label\n",
        "            class_samples = train_data[mask]\n",
        "\n",
        "            if len(class_samples) < 2:\n",
        "                continue\n",
        "\n",
        "            current_mean = torch.median(class_samples, dim=0)[0]\n",
        "            centered_data = class_samples - current_mean\n",
        "\n",
        "            current_cov = self._compute_robust_covariance(centered_data)\n",
        "            current_count = len(class_samples)\n",
        "\n",
        "            if prev_prototypes is not None and label.item() in prev_prototypes:\n",
        "                prev_count = prev_counts[label.item()]\n",
        "                total_count = current_count + prev_count\n",
        "                adaptive_alpha = 0.125 * current_count / total_count\n",
        "\n",
        "                previous_mean = prev_prototypes[label.item()]\n",
        "                combined_mean = adaptive_alpha * current_mean + (1 - adaptive_alpha) * previous_mean\n",
        "\n",
        "                previous_cov = prev_uncertainties[label.item()]\n",
        "                combined_cov = adaptive_alpha * current_cov + (1 - adaptive_alpha) * previous_cov\n",
        "\n",
        "                combined_count = current_count + prev_count\n",
        "            else:\n",
        "                combined_mean = current_mean\n",
        "                combined_cov = current_cov\n",
        "                combined_count = current_count\n",
        "\n",
        "            self.class_prototypes[label.item()] = combined_mean\n",
        "            self.class_uncertainties[label.item()] = combined_cov\n",
        "            self.class_counts[label.item()] = combined_count\n",
        "\n",
        "        return self.class_prototypes, self.class_uncertainties\n",
        "\n",
        "    def _compute_robust_covariance(self, centered_data: torch.Tensor) -> torch.Tensor:\n",
        "        n_samples = len(centered_data)\n",
        "\n",
        "        sample_cov = torch.mm(centered_data.T, centered_data) / (n_samples - 1)\n",
        "\n",
        "        distances = torch.sqrt(torch.sum(centered_data ** 2, dim=1))\n",
        "        weights = 1 / (1 + distances)\n",
        "        weighted_cov = (weights[:, None, None] * torch.bmm(centered_data[:, :, None], centered_data[:, None, :])).mean(0)\n",
        "\n",
        "        alpha = min(0.5, 1.0 / n_samples)\n",
        "\n",
        "        target = torch.diag(torch.diag(weighted_cov))\n",
        "\n",
        "        cov = (1 - alpha) * weighted_cov + alpha * target + self.reg_matrix\n",
        "        return cov\n",
        "\n",
        "    def compute_probability_batch(self, samples: torch.Tensor) -> torch.Tensor:\n",
        "        n_samples = len(samples)\n",
        "        n_classes = max(self.class_prototypes.keys()) + 1\n",
        "        log_probs = torch.full((n_samples, n_classes), float('-inf'), device=samples.device, dtype=samples.dtype)\n",
        "\n",
        "        for label in self.class_prototypes:\n",
        "            mean = self.class_prototypes[label]\n",
        "            cov = self.class_uncertainties[label]\n",
        "\n",
        "            try:\n",
        "                cov_pinv = torch.linalg.pinv(cov)\n",
        "                log_det = torch.logdet(cov + 1e-6 * torch.eye(cov.size(0), device=cov.device, dtype=cov.dtype))\n",
        "\n",
        "                diff = samples - mean.unsqueeze(0)\n",
        "                mahalanobis_dist = torch.einsum('bi,ij,bj->b', diff, cov_pinv, diff)\n",
        "\n",
        "                entropy = torch.sum(-torch.diagonal(cov) * torch.log(torch.diagonal(cov) + 1e-6))\n",
        "                total_entropy = sum(\n",
        "                    -torch.sum(torch.diagonal(self.class_uncertainties[cls]) * torch.log(torch.diagonal(self.class_uncertainties[cls]) + 1e-6))\n",
        "                    for cls in self.class_prototypes\n",
        "                )\n",
        "                class_prior = (total_entropy - entropy) / total_entropy\n",
        "\n",
        "                log_prior = torch.log(torch.tensor(class_prior, device=samples.device, dtype=samples.dtype))\n",
        "\n",
        "                log_probs[:, label] = -0.5 * (log_det + mahalanobis_dist + self.feature_dim * np.log(2 * np.pi)) + log_prior\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing label {label}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if torch.all(torch.isinf(log_probs)):\n",
        "            return torch.ones((n_samples, n_classes), device=samples.device, dtype=samples.dtype) / n_classes\n",
        "\n",
        "        log_probs = log_probs - torch.logsumexp(log_probs, dim=1, keepdim=True)\n",
        "        return torch.exp(log_probs)\n",
        "\n",
        "    def predict_proba(self, data: torch.Tensor) -> np.ndarray:\n",
        "        with torch.no_grad():\n",
        "            probs = self.compute_probability_batch(data)\n",
        "            probs = torch.nan_to_num(probs, 0.0)\n",
        "            probs = probs / torch.sum(probs, dim=1, keepdim=True)\n",
        "            return probs.numpy()\n",
        "\n",
        "    def predict(self, data: torch.Tensor) -> np.ndarray:\n",
        "        probs = self.predict_proba(data)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "    def get_uncertainty(self, data: torch.Tensor) -> np.ndarray:\n",
        "        probs = self.predict_proba(data)\n",
        "        probs = np.clip(probs, 1e-10, 1.0)\n",
        "        entropy = -np.sum(probs * np.log(probs), axis=1)\n",
        "        max_entropy = -np.log(1.0 / len(self.class_prototypes))\n",
        "        return entropy / max_entropy\n",
        "\n",
        "\n",
        "class UDASequentialTrainer:\n",
        "    def __init__(self, dataset_paths: list, heldout_paths: list,\n",
        "                 mmd_weight: float = 0.1,\n",
        "                 mmd_kernel: str = 'rbf'):\n",
        "        self.datasets = dataset_paths\n",
        "        self.mmd_weight = mmd_weight\n",
        "        self.mmd_kernel = mmd_kernel\n",
        "\n",
        "        self.heldout_datasets = heldout_paths\n",
        "        self.models = []\n",
        "        self.accuracies_matrix = np.zeros((10, 10))\n",
        "        self.uncertainties_matrix = np.zeros((10, 10))\n",
        "\n",
        "        self.feature_extractor = models.resnet34(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(self.feature_extractor.children())[:-1])\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.feature_extractor = self.feature_extractor.to(self.device)\n",
        "        self.feature_extractor.eval()\n",
        "\n",
        "        self.normalize = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "\n",
        "        print(f\"Using device: {self.device}\")\n",
        "    def compute_domain_adaptation_loss(self, source_features: torch.Tensor,\n",
        "                                        target_features: torch.Tensor) -> torch.Tensor:\n",
        "        return mmd_loss(source_features, target_features,\n",
        "                        kernel_type=self.mmd_kernel)\n",
        "\n",
        "    def preprocess_data(self, data):\n",
        "        if data.max() > 1:\n",
        "            data = data / 255.0\n",
        "        if data.shape[1:] == (32, 32, 3):\n",
        "            data = data.permute(0, 3, 1, 2)\n",
        "\n",
        "        if self.feature_extractor.training:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
        "            ])\n",
        "            data = torch.stack([transform(img) for img in data])\n",
        "\n",
        "        normalized_data = torch.stack([self.normalize(img) for img in data])\n",
        "        return normalized_data\n",
        "\n",
        "    def extract_features(self, data):\n",
        "        normalized_data = self.preprocess_data(data)\n",
        "        normalized_data = normalized_data.to(self.device)\n",
        "\n",
        "        features_list = []\n",
        "        batch_size = 64\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(normalized_data), batch_size):\n",
        "                batch = normalized_data[i:i + batch_size]\n",
        "                features = self.feature_extractor(batch)\n",
        "                features = features.view(features.size(0), -1)\n",
        "                features = F.normalize(features, p=2, dim=1)\n",
        "                features_list.append(features.cpu())\n",
        "\n",
        "        return torch.cat(features_list, dim=0)\n",
        "\n",
        "    def load_from_pth(self, path, is_eval=False):\n",
        "        try:\n",
        "            data_dict = torch.load(path)\n",
        "            data = torch.tensor(data_dict['data'], dtype=torch.float32)\n",
        "            targets = torch.tensor(data_dict['targets'], dtype=torch.long) if 'targets' in data_dict else None\n",
        "            print(f\"Loaded data from {path}: {data.shape}\")\n",
        "            return data, targets\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data from {path}: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "    def train_model_on_d1(self):\n",
        "        print(\"Training initial model on D1...\")\n",
        "        data, targets = self.load_from_pth(self.datasets[0])\n",
        "        if data is None or targets is None:\n",
        "            raise ValueError(\"Could not load initial training data\")\n",
        "\n",
        "        features = self.extract_features(data)\n",
        "        print(f\"Extracted features shape: {features.shape}\")\n",
        "\n",
        "        model = ProbabilisticLwPClassifier()\n",
        "        model.train(features, targets)\n",
        "        print(f\"Created initial prototypes for {len(model.class_prototypes)} classes\")\n",
        "        self.models.append(model)\n",
        "\n",
        "    def evaluate_model_on_data(self, model, data, targets):\n",
        "        if data is None or targets is None:\n",
        "            return 0.0, 1.0\n",
        "\n",
        "        features = self.extract_features(data)\n",
        "        predictions = model.predict(features)\n",
        "        accuracy = accuracy_score(targets, predictions)\n",
        "        uncertainties = model.get_uncertainty(features)\n",
        "        mean_uncertainty = np.mean(uncertainties)\n",
        "\n",
        "        return accuracy, mean_uncertainty\n",
        "\n",
        "\n",
        "\n",
        "    def run_sequential_training(self, mmd_weight=0.1):\n",
        "        print(\"Starting sequential probabilistic training with MMD adaptation...\")\n",
        "\n",
        "        if len(self.models) == 0:\n",
        "            self.train_model_on_d1()\n",
        "            self.evaluate_models()\n",
        "\n",
        "        for i in tqdm(range(1, len(self.datasets)), desc=\"Training on sequential datasets\"):\n",
        "            print(f\"\\nProcessing dataset {i+1}\")\n",
        "\n",
        "            data, targets = self.load_from_pth(self.datasets[i])\n",
        "            if data is None:\n",
        "                continue\n",
        "\n",
        "            current_features = self.extract_features(data)\n",
        "\n",
        "            current_model = self.models[-1]\n",
        "            previous_features = None\n",
        "\n",
        "            if i > 0:\n",
        "                prev_data, _ = self.load_from_pth(self.datasets[i-1])\n",
        "                if prev_data is not None:\n",
        "                    previous_features = self.extract_features(prev_data)\n",
        "\n",
        "            pseudo_labels = current_model.predict(current_features)\n",
        "            uncertainties = current_model.get_uncertainty(current_features)\n",
        "\n",
        "            confidence_threshold = np.percentile(uncertainties, 70)\n",
        "            confidence_mask = uncertainties < confidence_threshold\n",
        "\n",
        "            confident_features = current_features[confidence_mask]\n",
        "            confident_pseudo_labels = pseudo_labels[confidence_mask]\n",
        "\n",
        "            if len(confident_features) < 100:\n",
        "                print(f\"Warning: Insufficient confident predictions ({len(confident_features)})\")\n",
        "                confident_features = current_features\n",
        "                confident_pseudo_labels = pseudo_labels\n",
        "            else:\n",
        "                print(f\"Using {len(confident_features)}/{len(current_features)} confident predictions\")\n",
        "\n",
        "            mmd_loss = 0.0\n",
        "            if previous_features is not None:\n",
        "                mmd_loss = mmd_loss(\n",
        "                    source_features=previous_features,\n",
        "                    target_features=current_features,\n",
        "                    kernel_type='rbf'\n",
        "                )\n",
        "                print(f\"MMD Domain Adaptation Loss: {mmd_loss.item()}\")\n",
        "\n",
        "            new_model = ProbabilisticLwPClassifier()\n",
        "            new_model.train(\n",
        "                confident_features,\n",
        "                torch.tensor(confident_pseudo_labels),\n",
        "                prev_prototypes=current_model.class_prototypes,\n",
        "                prev_uncertainties=current_model.class_uncertainties,\n",
        "                prev_counts=current_model.class_counts\n",
        "            )\n",
        "\n",
        "            if mmd_loss > 0:\n",
        "                for label in new_model.class_prototypes:\n",
        "                    adjustment = mmd_weight * mmd_loss.item()\n",
        "                    new_model.class_prototypes[label] *= (1 - adjustment)\n",
        "\n",
        "            self.models.append(new_model)\n",
        "\n",
        "            self.evaluate_models()\n",
        "\n",
        "        self.plot_metrics()\n",
        "\n",
        "    def evaluate_models(self):\n",
        "        print(\"\\nEvaluating latest model...\")\n",
        "        current_model_index = len(self.models) - 1\n",
        "        model = self.models[-1]\n",
        "\n",
        "        for j in range(current_model_index + 1):\n",
        "            data, targets = self.load_from_pth(self.heldout_datasets[j], is_eval=True)\n",
        "            accuracy, uncertainty = self.evaluate_model_on_data(model, data, targets)\n",
        "            self.accuracies_matrix[current_model_index, j] = accuracy\n",
        "            self.uncertainties_matrix[current_model_index, j] = uncertainty\n",
        "            print(f\"Latest Model {current_model_index + 1} on Dataset {j + 1}:\")\n",
        "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"  Uncertainty: {uncertainty:.4f}\")\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(self.accuracies_matrix, annot=True, fmt='.3f', cmap='YlOrRd')\n",
        "        plt.title('Model Accuracy Matrix')\n",
        "        plt.xlabel('Dataset Index')\n",
        "        plt.ylabel('Model Index')\n",
        "        plt.savefig('accuracy_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(self.uncertainties_matrix, annot=True, fmt='.3f', cmap='YlOrRd')\n",
        "        plt.title('Model Uncertainty Matrix')\n",
        "        plt.xlabel('Dataset Index')\n",
        "        plt.ylabel('Model Index')\n",
        "        plt.savefig('uncertainty_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "class TaskTwoUDASequentialTrainer(UDASequentialTrainer):\n",
        "    def __init__(self, dataset_paths: list, heldout_paths: list):\n",
        "        super().__init__(dataset_paths, heldout_paths)\n",
        "\n",
        "        self.accuracies_matrix = np.zeros((11, 21))\n",
        "        self.uncertainties_matrix = np.zeros((11, 21))\n",
        "\n",
        "    def run_sequential_training(self, initial_model):\n",
        "        print(\"Starting sequential probabilistic training for Task 2...\")\n",
        "\n",
        "        self.models = [initial_model]\n",
        "        self.evaluate_models()\n",
        "\n",
        "        for i in tqdm(range(10, 20), desc=\"Training on sequential datasets\"):\n",
        "            print(f\"\\nProcessing dataset {i+1}\")\n",
        "            data, targets = self.load_from_pth(self.datasets[i])\n",
        "            if data is None:\n",
        "                continue\n",
        "\n",
        "            features = self.extract_features(data)\n",
        "            current_model = self.models[-1]\n",
        "\n",
        "            pseudo_labels = current_model.predict(features)\n",
        "            uncertainties = current_model.get_uncertainty(features)\n",
        "\n",
        "            confidence_threshold = np.percentile(uncertainties, 75)\n",
        "            confidence_mask = uncertainties < confidence_threshold\n",
        "            confident_features = features[confidence_mask]\n",
        "            confident_pseudo_labels = pseudo_labels[confidence_mask]\n",
        "\n",
        "            if len(confident_features) < 50:\n",
        "                print(f\"Warning: Very few confident predictions ({len(confident_features)})\")\n",
        "                confident_features = features\n",
        "                confident_pseudo_labels = pseudo_labels\n",
        "            else:\n",
        "                print(f\"Using {len(confident_features)}/{len(features)} confident predictions\")\n",
        "\n",
        "            new_model = ProbabilisticLwPClassifier()\n",
        "            new_model.train(\n",
        "                confident_features,\n",
        "                torch.tensor(confident_pseudo_labels),\n",
        "                prev_prototypes=current_model.class_prototypes,\n",
        "                prev_uncertainties=current_model.class_uncertainties,\n",
        "                prev_counts=current_model.class_counts\n",
        "            )\n",
        "            self.models.append(new_model)\n",
        "            self.evaluate_models()\n",
        "\n",
        "        self.plot_metrics()\n",
        "\n",
        "    def evaluate_models(self):\n",
        "        print(\"\\nEvaluating latest model...\")\n",
        "        current_model_index = len(self.models) - 1\n",
        "        model = self.models[-1]\n",
        "\n",
        "        for j in range(current_model_index + 10):\n",
        "            if j < 10:\n",
        "                data_path = f'/content/drive/MyDrive/dataset/part_one_dataset/eval_data/{j+1}_eval_data.tar.pth'\n",
        "            else:\n",
        "                data_path = f'/content/drive/MyDrive/dataset/part_two_dataset/eval_data/{j-9}_eval_data.tar.pth'\n",
        "\n",
        "            data, targets = self.load_from_pth(data_path, is_eval=True)\n",
        "            accuracy, uncertainty = self.evaluate_model_on_data(model, data, targets)\n",
        "            self.accuracies_matrix[current_model_index, j] = accuracy\n",
        "            self.uncertainties_matrix[current_model_index, j] = uncertainty\n",
        "\n",
        "            dataset_desc = f\"Part 1 Dataset {j+1}\" if j < 10 else f\"Part 2 Dataset {j-9}\"\n",
        "            print(f\"Latest Model {current_model_index + 1} on {dataset_desc}:\")\n",
        "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"  Uncertainty: {uncertainty:.4f}\")\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        max_used_col = 20\n",
        "        filled_accuracies = self.accuracies_matrix[:, :max_used_col]\n",
        "        filled_uncertainties = self.uncertainties_matrix[:, :max_used_col]\n",
        "\n",
        "        plt.figure(figsize=(20, 8))\n",
        "        x_labels = [f'Part 1 D{i+1}' for i in range(10)] + [f'Part 2 D{i+1}' for i in range(10)]\n",
        "        sns.heatmap(filled_accuracies, annot=True, fmt='.3f', cmap='YlOrRd',\n",
        "                    xticklabels=x_labels[:max_used_col])\n",
        "        plt.title('Model Accuracy Matrix for Task 2')\n",
        "        plt.xlabel('Dataset')\n",
        "        plt.ylabel('Model Index')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('task2_accuracy_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "        plt.figure(figsize=(20, 8))\n",
        "        sns.heatmap(filled_uncertainties, annot=True, fmt='.3f', cmap='YlOrRd',\n",
        "                    xticklabels=x_labels[:max_used_col])\n",
        "        plt.title('Model Uncertainty Matrix for Task 2')\n",
        "        plt.xlabel('Dataset')\n",
        "        plt.ylabel('Model Index')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('task2_uncertainty_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "def main():\n",
        "\n",
        "    part_1_train_paths = [f'/content/drive/MyDrive/dataset/part_one_dataset/train_data/{i}_train_data.tar.pth' for i in range(1, 11)]\n",
        "    part_1_eval_paths = [f'/content/drive/MyDrive/dataset/part_one_dataset/eval_data/{i}_eval_data.tar.pth' for i in range(1, 11)]\n",
        "\n",
        "    part_2_train_paths = [f'/content/drive/MyDrive/dataset/part_two_dataset/train_data/{i}_train_data.tar.pth' for i in range(1, 11)]\n",
        "    part_2_eval_paths = [f'/content/drive/MyDrive/dataset/part_two_dataset/eval_data/{i}_eval_data.tar.pth' for i in range(1, 11)]\n",
        "\n",
        "    dataset_paths = part_1_train_paths + part_2_train_paths\n",
        "    holdout_paths = part_1_eval_paths + part_2_eval_paths\n",
        "\n",
        "    initial_model = ProbabilisticLwPClassifier.load_model('f10.pth')\n",
        "\n",
        "    trainer = TaskTwoUDASequentialTrainer(dataset_paths, holdout_paths)\n",
        "\n",
        "    trainer.run_sequential_training(initial_model)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import accuracy_score\n",
        "from typing import List, Dict, Tuple\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "def mmd_loss(source_features: torch.Tensor, target_features: torch.Tensor,\n",
        "             kernel_type: str = 'rbf',\n",
        "             sigma: float = 1.0) -> torch.Tensor:\n",
        "    def gaussian_kernel(x, y, sigma=1.0):\n",
        "        x_size = x.size(0)\n",
        "        y_size = y.size(0)\n",
        "\n",
        "        # Compute squared pairwise distances\n",
        "        norm2 = torch.pow(x, 2).sum(1).unsqueeze(1) - 2 * torch.mm(x, y.t()) + torch.pow(y, 2).sum(1).unsqueeze(0)\n",
        "\n",
        "        # Compute kernel\n",
        "        return torch.exp(-norm2 / (2 * sigma * sigma))\n",
        "\n",
        "    def linear_kernel(x, y):\n",
        "        return torch.mm(x, y.t())\n",
        "\n",
        "    # Choose kernel\n",
        "    if kernel_type == 'rbf':\n",
        "        kernel = lambda x, y: gaussian_kernel(x, y, sigma)\n",
        "    else:\n",
        "        kernel = linear_kernel\n",
        "\n",
        "    # Compute kernel matrices\n",
        "    xx = kernel(source_features, source_features)\n",
        "    yy = kernel(target_features, target_features)\n",
        "    xy = kernel(source_features, target_features)\n",
        "\n",
        "    # Compute MMD\n",
        "    xx_mean = xx.mean()\n",
        "    yy_mean = yy.mean()\n",
        "    xy_mean = xy.mean()\n",
        "\n",
        "    return xx_mean + yy_mean - 2 * xy_mean\n",
        "\n",
        "def coral_loss(self, source, target):\n",
        "\n",
        "    d = source.size(1)\n",
        "\n",
        "    # Source covariance\n",
        "    source = source - torch.mean(source, dim=0, keepdim=True)\n",
        "    source_cov = torch.matmul(source.t(), source) / (source.size(0) - 1)\n",
        "\n",
        "\n",
        "    target = target - torch.mean(target, dim=0, keepdim=True)\n",
        "    target_cov = torch.matmul(target.t(), target) / (target.size(0) - 1)\n",
        "\n",
        "    # Frobenius norm between the two covariance matrices\n",
        "    loss = torch.norm(source_cov - target_cov, p='fro')\n",
        "    return loss / (4 * d * d)\n",
        "\n",
        "class ProbabilisticLwPClassifier:\n",
        "    def __init__(self, uncertainty_threshold=0.7, feature_dim=512):\n",
        "        self.class_prototypes = {}\n",
        "        self.class_uncertainties = {}\n",
        "        self.class_counts = {}\n",
        "        self.uncertainty_threshold = uncertainty_threshold\n",
        "        self.reg_matrix = torch.eye(feature_dim) * 1e-4\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, filepath):\n",
        "        drive_path = '/content/drive/MyDrive/ml_models/'\n",
        "        full_filepath = os.path.join(drive_path, filepath)\n",
        "\n",
        "        try:\n",
        "            model_state = torch.load(full_filepath)\n",
        "\n",
        "            loaded_model = cls(\n",
        "                uncertainty_threshold=model_state['uncertainty_threshold'],\n",
        "                feature_dim=model_state['feature_dim']\n",
        "            )\n",
        "\n",
        "            loaded_model.class_prototypes = {\n",
        "                k: torch.tensor(v) for k, v in model_state['class_prototypes'].items()\n",
        "            }\n",
        "\n",
        "            loaded_model.class_uncertainties = {\n",
        "                k: torch.tensor(v) for k, v in model_state['class_uncertainties'].items()\n",
        "            }\n",
        "\n",
        "            loaded_model.class_counts = model_state['class_counts']\n",
        "\n",
        "            print(f\"Model loaded successfully from {full_filepath}\")\n",
        "            return loaded_model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            return None\n",
        "\n",
        "    def train(self, train_data: torch.Tensor, train_labels: torch.Tensor,\n",
        "              prev_prototypes=None, prev_uncertainties=None, prev_counts=None):\n",
        "        unique_labels = torch.unique(train_labels)\n",
        "\n",
        "        for label in unique_labels:\n",
        "            mask = train_labels == label\n",
        "            class_samples = train_data[mask]\n",
        "\n",
        "            if len(class_samples) < 2:\n",
        "                continue\n",
        "\n",
        "            current_mean = torch.median(class_samples, dim=0)[0]\n",
        "            centered_data = class_samples - current_mean\n",
        "\n",
        "            current_cov = self._compute_robust_covariance(centered_data)\n",
        "            current_count = len(class_samples)\n",
        "\n",
        "            if prev_prototypes is not None and label.item() in prev_prototypes:\n",
        "                prev_count = prev_counts[label.item()]\n",
        "                total_count = current_count + prev_count\n",
        "                adaptive_alpha = 0.125 * current_count / total_count\n",
        "\n",
        "                previous_mean = prev_prototypes[label.item()]\n",
        "                combined_mean = adaptive_alpha * current_mean + (1 - adaptive_alpha) * previous_mean\n",
        "\n",
        "                previous_cov = prev_uncertainties[label.item()]\n",
        "                combined_cov = adaptive_alpha * current_cov + (1 - adaptive_alpha) * previous_cov\n",
        "\n",
        "                combined_count = current_count + prev_count\n",
        "            else:\n",
        "                combined_mean = current_mean\n",
        "                combined_cov = current_cov\n",
        "                combined_count = current_count\n",
        "\n",
        "            self.class_prototypes[label.item()] = combined_mean\n",
        "            self.class_uncertainties[label.item()] = combined_cov\n",
        "            self.class_counts[label.item()] = combined_count\n",
        "\n",
        "        return self.class_prototypes, self.class_uncertainties\n",
        "\n",
        "    def _compute_robust_covariance(self, centered_data: torch.Tensor) -> torch.Tensor:\n",
        "        n_samples = len(centered_data)\n",
        "\n",
        "        sample_cov = torch.mm(centered_data.T, centered_data) / (n_samples - 1)\n",
        "\n",
        "        distances = torch.sqrt(torch.sum(centered_data ** 2, dim=1))\n",
        "        weights = 1 / (1 + distances)\n",
        "        weighted_cov = (weights[:, None, None] * torch.bmm(centered_data[:, :, None], centered_data[:, None, :])).mean(0)\n",
        "\n",
        "        alpha = min(0.5, 1.0 / n_samples)\n",
        "\n",
        "        target = torch.diag(torch.diag(weighted_cov))\n",
        "\n",
        "        cov = (1 - alpha) * weighted_cov + alpha * target + self.reg_matrix\n",
        "        return cov\n",
        "\n",
        "    def compute_probability_batch(self, samples: torch.Tensor) -> torch.Tensor:\n",
        "        n_samples = len(samples)\n",
        "        n_classes = max(self.class_prototypes.keys()) + 1\n",
        "        log_probs = torch.full((n_samples, n_classes), float('-inf'), device=samples.device, dtype=samples.dtype)\n",
        "\n",
        "        for label in self.class_prototypes:\n",
        "            mean = self.class_prototypes[label]\n",
        "            cov = self.class_uncertainties[label]\n",
        "\n",
        "            try:\n",
        "                cov_pinv = torch.linalg.pinv(cov)\n",
        "                log_det = torch.logdet(cov + 1e-6 * torch.eye(cov.size(0), device=cov.device, dtype=cov.dtype))\n",
        "\n",
        "                diff = samples - mean.unsqueeze(0)\n",
        "                mahalanobis_dist = torch.einsum('bi,ij,bj->b', diff, cov_pinv, diff)\n",
        "\n",
        "                entropy = torch.sum(-torch.diagonal(cov) * torch.log(torch.diagonal(cov) + 1e-6))\n",
        "                total_entropy = sum(\n",
        "                    -torch.sum(torch.diagonal(self.class_uncertainties[cls]) * torch.log(torch.diagonal(self.class_uncertainties[cls]) + 1e-6))\n",
        "                    for cls in self.class_prototypes\n",
        "                )\n",
        "                class_prior = (total_entropy - entropy) / total_entropy\n",
        "\n",
        "                log_prior = torch.log(torch.tensor(class_prior, device=samples.device, dtype=samples.dtype))\n",
        "\n",
        "                log_probs[:, label] = -0.5 * (log_det + mahalanobis_dist + self.feature_dim * np.log(2 * np.pi)) + log_prior\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing label {label}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if torch.all(torch.isinf(log_probs)):\n",
        "            return torch.ones((n_samples, n_classes), device=samples.device, dtype=samples.dtype) / n_classes\n",
        "\n",
        "        log_probs = log_probs - torch.logsumexp(log_probs, dim=1, keepdim=True)\n",
        "        return torch.exp(log_probs)\n",
        "\n",
        "    def predict_proba(self, data: torch.Tensor) -> np.ndarray:\n",
        "        with torch.no_grad():\n",
        "            probs = self.compute_probability_batch(data)\n",
        "            probs = torch.nan_to_num(probs, 0.0)\n",
        "            probs = probs / torch.sum(probs, dim=1, keepdim=True)\n",
        "            return probs.numpy()\n",
        "\n",
        "    def predict(self, data: torch.Tensor) -> np.ndarray:\n",
        "        probs = self.predict_proba(data)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "    def get_uncertainty(self, data: torch.Tensor) -> np.ndarray:\n",
        "        probs = self.predict_proba(data)\n",
        "        probs = np.clip(probs, 1e-10, 1.0)\n",
        "        entropy = -np.sum(probs * np.log(probs), axis=1)\n",
        "        max_entropy = -np.log(1.0 / len(self.class_prototypes))\n",
        "        return entropy / max_entropy\n",
        "\n",
        "\n",
        "class UDASequentialTrainer:\n",
        "    def __init__(self, dataset_paths: list, heldout_paths: list,\n",
        "                 mmd_weight: float = 0.1,\n",
        "                 mmd_kernel: str = 'rbf'):\n",
        "        self.datasets = dataset_paths\n",
        "        self.mmd_weight = mmd_weight\n",
        "        self.mmd_kernel = mmd_kernel\n",
        "\n",
        "        self.heldout_datasets = heldout_paths\n",
        "        self.models = []\n",
        "        self.accuracies_matrix = np.zeros((10, 10))\n",
        "        self.uncertainties_matrix = np.zeros((10, 10))\n",
        "\n",
        "        self.feature_extractor = models.resnet34(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(self.feature_extractor.children())[:-1])\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.feature_extractor = self.feature_extractor.to(self.device)\n",
        "        self.feature_extractor.eval()\n",
        "\n",
        "        self.normalize = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "\n",
        "        print(f\"Using device: {self.device}\")\n",
        "    def compute_domain_adaptation_loss(self, source_features: torch.Tensor,\n",
        "                                        target_features: torch.Tensor) -> torch.Tensor:\n",
        "        return mmd_loss(source_features, target_features,\n",
        "                        kernel_type=self.mmd_kernel)\n",
        "\n",
        "    def preprocess_data(self, data):\n",
        "        if data.max() > 1:\n",
        "            data = data / 255.0\n",
        "        if data.shape[1:] == (32, 32, 3):\n",
        "            data = data.permute(0, 3, 1, 2)\n",
        "\n",
        "        if self.feature_extractor.training:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
        "            ])\n",
        "            data = torch.stack([transform(img) for img in data])\n",
        "\n",
        "        normalized_data = torch.stack([self.normalize(img) for img in data])\n",
        "        return normalized_data\n",
        "\n",
        "    def extract_features(self, data):\n",
        "        normalized_data = self.preprocess_data(data)\n",
        "        normalized_data = normalized_data.to(self.device)\n",
        "\n",
        "        features_list = []\n",
        "        batch_size = 64\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(normalized_data), batch_size):\n",
        "                batch = normalized_data[i:i + batch_size]\n",
        "                features = self.feature_extractor(batch)\n",
        "                features = features.view(features.size(0), -1)\n",
        "                features = F.normalize(features, p=2, dim=1)\n",
        "                features_list.append(features.cpu())\n",
        "\n",
        "        return torch.cat(features_list, dim=0)\n",
        "\n",
        "    def load_from_pth(self, path, is_eval=False):\n",
        "        try:\n",
        "            data_dict = torch.load(path)\n",
        "            data = torch.tensor(data_dict['data'], dtype=torch.float32)\n",
        "            targets = torch.tensor(data_dict['targets'], dtype=torch.long) if 'targets' in data_dict else None\n",
        "            print(f\"Loaded data from {path}: {data.shape}\")\n",
        "            return data, targets\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data from {path}: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "    def train_model_on_d1(self):\n",
        "        print(\"Training initial model on D1...\")\n",
        "        data, targets = self.load_from_pth(self.datasets[0])\n",
        "        if data is None or targets is None:\n",
        "            raise ValueError(\"Could not load initial training data\")\n",
        "\n",
        "        features = self.extract_features(data)\n",
        "        print(f\"Extracted features shape: {features.shape}\")\n",
        "\n",
        "        model = ProbabilisticLwPClassifier()\n",
        "        model.train(features, targets)\n",
        "        print(f\"Created initial prototypes for {len(model.class_prototypes)} classes\")\n",
        "        self.models.append(model)\n",
        "\n",
        "    def evaluate_model_on_data(self, model, data, targets):\n",
        "        if data is None or targets is None:\n",
        "            return 0.0, 1.0\n",
        "\n",
        "        features = self.extract_features(data)\n",
        "        predictions = model.predict(features)\n",
        "        accuracy = accuracy_score(targets, predictions)\n",
        "        uncertainties = model.get_uncertainty(features)\n",
        "        mean_uncertainty = np.mean(uncertainties)\n",
        "\n",
        "        return accuracy, mean_uncertainty\n",
        "\n",
        "\n",
        "\n",
        "    def run_sequential_training(self, mmd_weight=0.1):\n",
        "        print(\"Starting sequential probabilistic training with MMD adaptation...\")\n",
        "\n",
        "        if len(self.models) == 0:\n",
        "            self.train_model_on_d1()\n",
        "            self.evaluate_models()\n",
        "\n",
        "        for i in tqdm(range(1, len(self.datasets)), desc=\"Training on sequential datasets\"):\n",
        "            print(f\"\\nProcessing dataset {i+1}\")\n",
        "\n",
        "            data, targets = self.load_from_pth(self.datasets[i])\n",
        "            if data is None:\n",
        "                continue\n",
        "\n",
        "            current_features = self.extract_features(data)\n",
        "\n",
        "            current_model = self.models[-1]\n",
        "            previous_features = None\n",
        "\n",
        "            if i > 0:\n",
        "                prev_data, _ = self.load_from_pth(self.datasets[i-1])\n",
        "                if prev_data is not None:\n",
        "                    previous_features = self.extract_features(prev_data)\n",
        "\n",
        "            pseudo_labels = current_model.predict(current_features)\n",
        "            uncertainties = current_model.get_uncertainty(current_features)\n",
        "\n",
        "            confidence_threshold = np.percentile(uncertainties, 70)\n",
        "            confidence_mask = uncertainties < confidence_threshold\n",
        "\n",
        "            confident_features = current_features[confidence_mask]\n",
        "            confident_pseudo_labels = pseudo_labels[confidence_mask]\n",
        "\n",
        "            if len(confident_features) < 100:\n",
        "                print(f\"Warning: Insufficient confident predictions ({len(confident_features)})\")\n",
        "                confident_features = current_features\n",
        "                confident_pseudo_labels = pseudo_labels\n",
        "            else:\n",
        "                print(f\"Using {len(confident_features)}/{len(current_features)} confident predictions\")\n",
        "\n",
        "            mmd_loss = 0.0\n",
        "            if previous_features is not None:\n",
        "                mmd_loss = mmd_loss(\n",
        "                    source_features=previous_features,\n",
        "                    target_features=current_features,\n",
        "                    kernel_type='rbf'\n",
        "                )\n",
        "                print(f\"MMD Domain Adaptation Loss: {mmd_loss.item()}\")\n",
        "\n",
        "            new_model = ProbabilisticLwPClassifier()\n",
        "            new_model.train(\n",
        "                confident_features,\n",
        "                torch.tensor(confident_pseudo_labels),\n",
        "                prev_prototypes=current_model.class_prototypes,\n",
        "                prev_uncertainties=current_model.class_uncertainties,\n",
        "                prev_counts=current_model.class_counts\n",
        "            )\n",
        "\n",
        "            if mmd_loss > 0:\n",
        "                for label in new_model.class_prototypes:\n",
        "                    adjustment = mmd_weight * mmd_loss.item()\n",
        "                    new_model.class_prototypes[label] *= (1 - adjustment)\n",
        "\n",
        "            self.models.append(new_model)\n",
        "\n",
        "            self.evaluate_models()\n",
        "\n",
        "        self.plot_metrics()\n",
        "\n",
        "    def evaluate_models(self):\n",
        "        print(\"\\nEvaluating latest model...\")\n",
        "        current_model_index = len(self.models) - 1\n",
        "        model = self.models[-1]\n",
        "\n",
        "        for j in range(current_model_index + 1):\n",
        "            data, targets = self.load_from_pth(self.heldout_datasets[j], is_eval=True)\n",
        "            accuracy, uncertainty = self.evaluate_model_on_data(model, data, targets)\n",
        "            self.accuracies_matrix[current_model_index, j] = accuracy\n",
        "            self.uncertainties_matrix[current_model_index, j] = uncertainty\n",
        "            print(f\"Latest Model {current_model_index + 1} on Dataset {j + 1}:\")\n",
        "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"  Uncertainty: {uncertainty:.4f}\")\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(self.accuracies_matrix, annot=True, fmt='.3f', cmap='YlOrRd')\n",
        "        plt.title('Model Accuracy Matrix')\n",
        "        plt.xlabel('Dataset Index')\n",
        "        plt.ylabel('Model Index')\n",
        "        plt.savefig('accuracy_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(self.uncertainties_matrix, annot=True, fmt='.3f', cmap='YlOrRd')\n",
        "        plt.title('Model Uncertainty Matrix')\n",
        "        plt.xlabel('Dataset Index')\n",
        "        plt.ylabel('Model Index')\n",
        "        plt.savefig('uncertainty_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "class TaskTwoUDASequentialTrainer(UDASequentialTrainer):\n",
        "    def __init__(self, dataset_paths: list, heldout_paths: list):\n",
        "        super().__init__(dataset_paths, heldout_paths)\n",
        "\n",
        "        self.accuracies_matrix = np.zeros((11, 21))\n",
        "        self.uncertainties_matrix = np.zeros((11, 21))\n",
        "\n",
        "    def run_sequential_training(self, initial_model):\n",
        "        print(\"Starting sequential probabilistic training for Task 2...\")\n",
        "\n",
        "        self.models = [initial_model]\n",
        "        self.evaluate_models()\n",
        "\n",
        "        for i in tqdm(range(10, 20), desc=\"Training on sequential datasets\"):\n",
        "            print(f\"\\nProcessing dataset {i+1}\")\n",
        "            data, targets = self.load_from_pth(self.datasets[i])\n",
        "            if data is None:\n",
        "                continue\n",
        "\n",
        "            features = self.extract_features(data)\n",
        "            current_model = self.models[-1]\n",
        "\n",
        "            pseudo_labels = current_model.predict(features)\n",
        "            uncertainties = current_model.get_uncertainty(features)\n",
        "\n",
        "            confidence_threshold = np.percentile(uncertainties, 75)\n",
        "            confidence_mask = uncertainties < confidence_threshold\n",
        "            confident_features = features[confidence_mask]\n",
        "            confident_pseudo_labels = pseudo_labels[confidence_mask]\n",
        "\n",
        "            if len(confident_features) < 50:\n",
        "                print(f\"Warning: Very few confident predictions ({len(confident_features)})\")\n",
        "                confident_features = features\n",
        "                confident_pseudo_labels = pseudo_labels\n",
        "            else:\n",
        "                print(f\"Using {len(confident_features)}/{len(features)} confident predictions\")\n",
        "\n",
        "            new_model = ProbabilisticLwPClassifier()\n",
        "            new_model.train(\n",
        "                confident_features,\n",
        "                torch.tensor(confident_pseudo_labels),\n",
        "                prev_prototypes=current_model.class_prototypes,\n",
        "                prev_uncertainties=current_model.class_uncertainties,\n",
        "                prev_counts=current_model.class_counts\n",
        "            )\n",
        "            self.models.append(new_model)\n",
        "            self.evaluate_models()\n",
        "\n",
        "        self.plot_metrics()\n",
        "\n",
        "    def evaluate_models(self):\n",
        "        print(\"\\nEvaluating latest model...\")\n",
        "        current_model_index = len(self.models) - 1\n",
        "        model = self.models[-1]\n",
        "\n",
        "        for j in range(current_model_index + 10):\n",
        "            if j < 10:\n",
        "                data_path = f'/content/drive/MyDrive/dataset/part_one_dataset/eval_data/{j+1}_eval_data.tar.pth'\n",
        "            else:\n",
        "                data_path = f'/content/drive/MyDrive/dataset/part_two_dataset/eval_data/{j-9}_eval_data.tar.pth'\n",
        "\n",
        "            data, targets = self.load_from_pth(data_path, is_eval=True)\n",
        "            accuracy, uncertainty = self.evaluate_model_on_data(model, data, targets)\n",
        "            self.accuracies_matrix[current_model_index, j] = accuracy\n",
        "            self.uncertainties_matrix[current_model_index, j] = uncertainty\n",
        "\n",
        "            dataset_desc = f\"Part 1 Dataset {j+1}\" if j < 10 else f\"Part 2 Dataset {j-9}\"\n",
        "            print(f\"Latest Model {current_model_index + 1} on {dataset_desc}:\")\n",
        "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"  Uncertainty: {uncertainty:.4f}\")\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        max_used_col = 20\n",
        "        filled_accuracies = self.accuracies_matrix[:, :max_used_col]\n",
        "        filled_uncertainties = self.uncertainties_matrix[:, :max_used_col]\n",
        "\n",
        "        plt.figure(figsize=(20, 8))\n",
        "        x_labels = [f'Part 1 D{i+1}' for i in range(10)] + [f'Part 2 D{i+1}' for i in range(10)]\n",
        "        sns.heatmap(filled_accuracies, annot=True, fmt='.3f', cmap='YlOrRd',\n",
        "                    xticklabels=x_labels[:max_used_col])\n",
        "        plt.title('Model Accuracy Matrix for Task 2')\n",
        "        plt.xlabel('Dataset')\n",
        "        plt.ylabel('Model Index')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('task2_accuracy_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "        plt.figure(figsize=(20, 8))\n",
        "        sns.heatmap(filled_uncertainties, annot=True, fmt='.3f', cmap='YlOrRd',\n",
        "                    xticklabels=x_labels[:max_used_col])\n",
        "        plt.title('Model Uncertainty Matrix for Task 2')\n",
        "        plt.xlabel('Dataset')\n",
        "        plt.ylabel('Model Index')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('task2_uncertainty_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "def main():\n",
        "\n",
        "    part_1_train_paths = [f'/content/drive/MyDrive/dataset/part_one_dataset/train_data/{i}_train_data.tar.pth' for i in range(1, 11)]\n",
        "    part_1_eval_paths = [f'/content/drive/MyDrive/dataset/part_one_dataset/eval_data/{i}_eval_data.tar.pth' for i in range(1, 11)]\n",
        "\n",
        "    part_2_train_paths = [f'/content/drive/MyDrive/dataset/part_two_dataset/train_data/{i}_train_data.tar.pth' for i in range(1, 11)]\n",
        "    part_2_eval_paths = [f'/content/drive/MyDrive/dataset/part_two_dataset/eval_data/{i}_eval_data.tar.pth' for i in range(1, 11)]\n",
        "\n",
        "    dataset_paths = part_1_train_paths + part_2_train_paths\n",
        "    holdout_paths = part_1_eval_paths + part_2_eval_paths\n",
        "\n",
        "    initial_model = ProbabilisticLwPClassifier.load_model('f10.pth')\n",
        "\n",
        "    trainer = TaskTwoUDASequentialTrainer(dataset_paths, holdout_paths)\n",
        "\n",
        "    trainer.run_sequential_training(initial_model)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WniqZPubyqo",
        "outputId": "d58f9fb0-30da-4636-b52f-cbece26f105b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Model loaded successfully from /content/drive/MyDrive/ml_models/f10.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|| 83.3M/83.3M [00:00<00:00, 134MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Starting sequential probabilistic training for Task 2...\n",
            "\n",
            "Evaluating latest model...\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 1 on Part 1 Dataset 1:\n",
            "  Accuracy: 0.5936\n",
            "  Uncertainty: 0.0128\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 1 on Part 1 Dataset 2:\n",
            "  Accuracy: 0.5888\n",
            "  Uncertainty: 0.0135\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 1 on Part 1 Dataset 3:\n",
            "  Accuracy: 0.5816\n",
            "  Uncertainty: 0.0115\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 1 on Part 1 Dataset 4:\n",
            "  Accuracy: 0.5828\n",
            "  Uncertainty: 0.0141\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 1 on Part 1 Dataset 5:\n",
            "  Accuracy: 0.5924\n",
            "  Uncertainty: 0.0118\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 1 on Part 1 Dataset 6:\n",
            "  Accuracy: 0.5812\n",
            "  Uncertainty: 0.0120\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 1 on Part 1 Dataset 7:\n",
            "  Accuracy: 0.5728\n",
            "  Uncertainty: 0.0126\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 1 on Part 1 Dataset 8:\n",
            "  Accuracy: 0.5824\n",
            "  Uncertainty: 0.0136\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 1 on Part 1 Dataset 9:\n",
            "  Accuracy: 0.5792\n",
            "  Uncertainty: 0.0121\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 1 on Part 1 Dataset 10:\n",
            "  Accuracy: 0.5828\n",
            "  Uncertainty: 0.0130\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining on sequential datasets:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing dataset 11\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/train_data/1_train_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Using 1875/2500 confident predictions\n",
            "\n",
            "Evaluating latest model...\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 2 on Part 1 Dataset 1:\n",
            "  Accuracy: 0.5928\n",
            "  Uncertainty: 0.0133\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 2 on Part 1 Dataset 2:\n",
            "  Accuracy: 0.5884\n",
            "  Uncertainty: 0.0134\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 2 on Part 1 Dataset 3:\n",
            "  Accuracy: 0.5808\n",
            "  Uncertainty: 0.0117\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 2 on Part 1 Dataset 4:\n",
            "  Accuracy: 0.5816\n",
            "  Uncertainty: 0.0143\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 2 on Part 1 Dataset 5:\n",
            "  Accuracy: 0.5920\n",
            "  Uncertainty: 0.0116\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 2 on Part 1 Dataset 6:\n",
            "  Accuracy: 0.5812\n",
            "  Uncertainty: 0.0121\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 2 on Part 1 Dataset 7:\n",
            "  Accuracy: 0.5704\n",
            "  Uncertainty: 0.0127\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 2 on Part 1 Dataset 8:\n",
            "  Accuracy: 0.5808\n",
            "  Uncertainty: 0.0136\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 2 on Part 1 Dataset 9:\n",
            "  Accuracy: 0.5792\n",
            "  Uncertainty: 0.0122\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 2 on Part 1 Dataset 10:\n",
            "  Accuracy: 0.5820\n",
            "  Uncertainty: 0.0131\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining on sequential datasets:  10%|         | 1/10 [02:48<25:18, 168.67s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latest Model 2 on Part 2 Dataset 1:\n",
            "  Accuracy: 0.5400\n",
            "  Uncertainty: 0.0132\n",
            "\n",
            "Processing dataset 12\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/train_data/2_train_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Using 1875/2500 confident predictions\n",
            "\n",
            "Evaluating latest model...\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 3 on Part 1 Dataset 1:\n",
            "  Accuracy: 0.5924\n",
            "  Uncertainty: 0.0130\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 3 on Part 1 Dataset 2:\n",
            "  Accuracy: 0.5860\n",
            "  Uncertainty: 0.0136\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 3 on Part 1 Dataset 3:\n",
            "  Accuracy: 0.5804\n",
            "  Uncertainty: 0.0118\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 3 on Part 1 Dataset 4:\n",
            "  Accuracy: 0.5828\n",
            "  Uncertainty: 0.0145\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 3 on Part 1 Dataset 5:\n",
            "  Accuracy: 0.5900\n",
            "  Uncertainty: 0.0120\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 3 on Part 1 Dataset 6:\n",
            "  Accuracy: 0.5808\n",
            "  Uncertainty: 0.0122\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 3 on Part 1 Dataset 7:\n",
            "  Accuracy: 0.5692\n",
            "  Uncertainty: 0.0126\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 3 on Part 1 Dataset 8:\n",
            "  Accuracy: 0.5808\n",
            "  Uncertainty: 0.0140\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 3 on Part 1 Dataset 9:\n",
            "  Accuracy: 0.5792\n",
            "  Uncertainty: 0.0116\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 3 on Part 1 Dataset 10:\n",
            "  Accuracy: 0.5816\n",
            "  Uncertainty: 0.0133\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 3 on Part 2 Dataset 1:\n",
            "  Accuracy: 0.5408\n",
            "  Uncertainty: 0.0133\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining on sequential datasets:  20%|        | 2/10 [05:53<23:45, 178.15s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latest Model 3 on Part 2 Dataset 2:\n",
            "  Accuracy: 0.4528\n",
            "  Uncertainty: 0.0150\n",
            "\n",
            "Processing dataset 13\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/train_data/3_train_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Using 1875/2500 confident predictions\n",
            "\n",
            "Evaluating latest model...\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 4 on Part 1 Dataset 1:\n",
            "  Accuracy: 0.5928\n",
            "  Uncertainty: 0.0130\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 4 on Part 1 Dataset 2:\n",
            "  Accuracy: 0.5876\n",
            "  Uncertainty: 0.0136\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 4 on Part 1 Dataset 3:\n",
            "  Accuracy: 0.5780\n",
            "  Uncertainty: 0.0118\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 4 on Part 1 Dataset 4:\n",
            "  Accuracy: 0.5812\n",
            "  Uncertainty: 0.0145\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 4 on Part 1 Dataset 5:\n",
            "  Accuracy: 0.5896\n",
            "  Uncertainty: 0.0117\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 4 on Part 1 Dataset 6:\n",
            "  Accuracy: 0.5804\n",
            "  Uncertainty: 0.0116\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 4 on Part 1 Dataset 7:\n",
            "  Accuracy: 0.5692\n",
            "  Uncertainty: 0.0131\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 4 on Part 1 Dataset 8:\n",
            "  Accuracy: 0.5804\n",
            "  Uncertainty: 0.0146\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 4 on Part 1 Dataset 9:\n",
            "  Accuracy: 0.5780\n",
            "  Uncertainty: 0.0116\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 4 on Part 1 Dataset 10:\n",
            "  Accuracy: 0.5804\n",
            "  Uncertainty: 0.0137\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 4 on Part 2 Dataset 1:\n",
            "  Accuracy: 0.5392\n",
            "  Uncertainty: 0.0138\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 4 on Part 2 Dataset 2:\n",
            "  Accuracy: 0.4512\n",
            "  Uncertainty: 0.0151\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining on sequential datasets:  30%|       | 3/10 [09:09<21:45, 186.54s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latest Model 4 on Part 2 Dataset 3:\n",
            "  Accuracy: 0.5040\n",
            "  Uncertainty: 0.0135\n",
            "\n",
            "Processing dataset 14\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/train_data/4_train_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Using 1875/2500 confident predictions\n",
            "\n",
            "Evaluating latest model...\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 1 Dataset 1:\n",
            "  Accuracy: 0.5928\n",
            "  Uncertainty: 0.0133\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 1 Dataset 2:\n",
            "  Accuracy: 0.5868\n",
            "  Uncertainty: 0.0130\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 1 Dataset 3:\n",
            "  Accuracy: 0.5776\n",
            "  Uncertainty: 0.0117\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 1 Dataset 4:\n",
            "  Accuracy: 0.5812\n",
            "  Uncertainty: 0.0148\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 1 Dataset 5:\n",
            "  Accuracy: 0.5884\n",
            "  Uncertainty: 0.0116\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 1 Dataset 6:\n",
            "  Accuracy: 0.5796\n",
            "  Uncertainty: 0.0114\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 1 Dataset 7:\n",
            "  Accuracy: 0.5704\n",
            "  Uncertainty: 0.0133\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 1 Dataset 8:\n",
            "  Accuracy: 0.5804\n",
            "  Uncertainty: 0.0148\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 1 Dataset 9:\n",
            "  Accuracy: 0.5780\n",
            "  Uncertainty: 0.0118\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 1 Dataset 10:\n",
            "  Accuracy: 0.5796\n",
            "  Uncertainty: 0.0140\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 2 Dataset 1:\n",
            "  Accuracy: 0.5384\n",
            "  Uncertainty: 0.0142\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 2 Dataset 2:\n",
            "  Accuracy: 0.4512\n",
            "  Uncertainty: 0.0152\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 5 on Part 2 Dataset 3:\n",
            "  Accuracy: 0.5036\n",
            "  Uncertainty: 0.0130\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining on sequential datasets:  40%|      | 4/10 [12:37<19:29, 195.00s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latest Model 5 on Part 2 Dataset 4:\n",
            "  Accuracy: 0.4460\n",
            "  Uncertainty: 0.0114\n",
            "\n",
            "Processing dataset 15\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/train_data/5_train_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Using 1875/2500 confident predictions\n",
            "\n",
            "Evaluating latest model...\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 1 Dataset 1:\n",
            "  Accuracy: 0.5924\n",
            "  Uncertainty: 0.0135\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 1 Dataset 2:\n",
            "  Accuracy: 0.5868\n",
            "  Uncertainty: 0.0130\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 1 Dataset 3:\n",
            "  Accuracy: 0.5780\n",
            "  Uncertainty: 0.0118\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 1 Dataset 4:\n",
            "  Accuracy: 0.5804\n",
            "  Uncertainty: 0.0149\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 1 Dataset 5:\n",
            "  Accuracy: 0.5880\n",
            "  Uncertainty: 0.0116\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 1 Dataset 6:\n",
            "  Accuracy: 0.5800\n",
            "  Uncertainty: 0.0113\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 1 Dataset 7:\n",
            "  Accuracy: 0.5692\n",
            "  Uncertainty: 0.0134\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 1 Dataset 8:\n",
            "  Accuracy: 0.5820\n",
            "  Uncertainty: 0.0147\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 1 Dataset 9:\n",
            "  Accuracy: 0.5780\n",
            "  Uncertainty: 0.0118\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 1 Dataset 10:\n",
            "  Accuracy: 0.5808\n",
            "  Uncertainty: 0.0144\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 2 Dataset 1:\n",
            "  Accuracy: 0.5388\n",
            "  Uncertainty: 0.0142\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 2 Dataset 2:\n",
            "  Accuracy: 0.4508\n",
            "  Uncertainty: 0.0152\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 2 Dataset 3:\n",
            "  Accuracy: 0.5020\n",
            "  Uncertainty: 0.0131\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 6 on Part 2 Dataset 4:\n",
            "  Accuracy: 0.4484\n",
            "  Uncertainty: 0.0114\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining on sequential datasets:  50%|     | 5/10 [16:21<17:07, 205.45s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latest Model 6 on Part 2 Dataset 5:\n",
            "  Accuracy: 0.5952\n",
            "  Uncertainty: 0.0113\n",
            "\n",
            "Processing dataset 16\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/train_data/6_train_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Using 1875/2500 confident predictions\n",
            "\n",
            "Evaluating latest model...\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 1 Dataset 1:\n",
            "  Accuracy: 0.5920\n",
            "  Uncertainty: 0.0132\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 1 Dataset 2:\n",
            "  Accuracy: 0.5852\n",
            "  Uncertainty: 0.0133\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 1 Dataset 3:\n",
            "  Accuracy: 0.5776\n",
            "  Uncertainty: 0.0117\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 1 Dataset 4:\n",
            "  Accuracy: 0.5784\n",
            "  Uncertainty: 0.0150\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 1 Dataset 5:\n",
            "  Accuracy: 0.5884\n",
            "  Uncertainty: 0.0115\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 1 Dataset 6:\n",
            "  Accuracy: 0.5804\n",
            "  Uncertainty: 0.0112\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 1 Dataset 7:\n",
            "  Accuracy: 0.5696\n",
            "  Uncertainty: 0.0132\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 1 Dataset 8:\n",
            "  Accuracy: 0.5812\n",
            "  Uncertainty: 0.0146\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 1 Dataset 9:\n",
            "  Accuracy: 0.5784\n",
            "  Uncertainty: 0.0116\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 1 Dataset 10:\n",
            "  Accuracy: 0.5800\n",
            "  Uncertainty: 0.0146\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 2 Dataset 1:\n",
            "  Accuracy: 0.5388\n",
            "  Uncertainty: 0.0141\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 2 Dataset 2:\n",
            "  Accuracy: 0.4492\n",
            "  Uncertainty: 0.0152\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 2 Dataset 3:\n",
            "  Accuracy: 0.5028\n",
            "  Uncertainty: 0.0131\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 2 Dataset 4:\n",
            "  Accuracy: 0.4480\n",
            "  Uncertainty: 0.0113\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 7 on Part 2 Dataset 5:\n",
            "  Accuracy: 0.5940\n",
            "  Uncertainty: 0.0113\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining on sequential datasets:  60%|    | 6/10 [20:19<14:25, 216.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest Model 7 on Part 2 Dataset 6:\n",
            "  Accuracy: 0.4780\n",
            "  Uncertainty: 0.0145\n",
            "\n",
            "Processing dataset 17\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/train_data/7_train_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Using 1875/2500 confident predictions\n",
            "\n",
            "Evaluating latest model...\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 1 Dataset 1:\n",
            "  Accuracy: 0.5924\n",
            "  Uncertainty: 0.0131\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 1 Dataset 2:\n",
            "  Accuracy: 0.5848\n",
            "  Uncertainty: 0.0130\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 1 Dataset 3:\n",
            "  Accuracy: 0.5776\n",
            "  Uncertainty: 0.0118\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 1 Dataset 4:\n",
            "  Accuracy: 0.5792\n",
            "  Uncertainty: 0.0148\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 1 Dataset 5:\n",
            "  Accuracy: 0.5884\n",
            "  Uncertainty: 0.0115\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 1 Dataset 6:\n",
            "  Accuracy: 0.5816\n",
            "  Uncertainty: 0.0113\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 1 Dataset 7:\n",
            "  Accuracy: 0.5704\n",
            "  Uncertainty: 0.0129\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 1 Dataset 8:\n",
            "  Accuracy: 0.5784\n",
            "  Uncertainty: 0.0146\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 1 Dataset 9:\n",
            "  Accuracy: 0.5784\n",
            "  Uncertainty: 0.0118\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 1 Dataset 10:\n",
            "  Accuracy: 0.5796\n",
            "  Uncertainty: 0.0145\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 2 Dataset 1:\n",
            "  Accuracy: 0.5384\n",
            "  Uncertainty: 0.0139\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 2 Dataset 2:\n",
            "  Accuracy: 0.4500\n",
            "  Uncertainty: 0.0151\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 2 Dataset 3:\n",
            "  Accuracy: 0.5020\n",
            "  Uncertainty: 0.0129\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 2 Dataset 4:\n",
            "  Accuracy: 0.4476\n",
            "  Uncertainty: 0.0111\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 2 Dataset 5:\n",
            "  Accuracy: 0.5936\n",
            "  Uncertainty: 0.0115\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 8 on Part 2 Dataset 6:\n",
            "  Accuracy: 0.4780\n",
            "  Uncertainty: 0.0144\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining on sequential datasets:  70%|   | 7/10 [24:30<11:22, 227.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest Model 8 on Part 2 Dataset 7:\n",
            "  Accuracy: 0.4552\n",
            "  Uncertainty: 0.0118\n",
            "\n",
            "Processing dataset 18\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/train_data/8_train_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Using 1875/2500 confident predictions\n",
            "\n",
            "Evaluating latest model...\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 1 Dataset 1:\n",
            "  Accuracy: 0.5924\n",
            "  Uncertainty: 0.0132\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 1 Dataset 2:\n",
            "  Accuracy: 0.5852\n",
            "  Uncertainty: 0.0132\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 1 Dataset 3:\n",
            "  Accuracy: 0.5768\n",
            "  Uncertainty: 0.0118\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 1 Dataset 4:\n",
            "  Accuracy: 0.5812\n",
            "  Uncertainty: 0.0148\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 1 Dataset 5:\n",
            "  Accuracy: 0.5880\n",
            "  Uncertainty: 0.0115\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 1 Dataset 6:\n",
            "  Accuracy: 0.5812\n",
            "  Uncertainty: 0.0111\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 1 Dataset 7:\n",
            "  Accuracy: 0.5700\n",
            "  Uncertainty: 0.0129\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 1 Dataset 8:\n",
            "  Accuracy: 0.5784\n",
            "  Uncertainty: 0.0146\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 1 Dataset 9:\n",
            "  Accuracy: 0.5776\n",
            "  Uncertainty: 0.0117\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 1 Dataset 10:\n",
            "  Accuracy: 0.5788\n",
            "  Uncertainty: 0.0143\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 2 Dataset 1:\n",
            "  Accuracy: 0.5380\n",
            "  Uncertainty: 0.0138\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 2 Dataset 2:\n",
            "  Accuracy: 0.4492\n",
            "  Uncertainty: 0.0154\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 2 Dataset 3:\n",
            "  Accuracy: 0.5008\n",
            "  Uncertainty: 0.0129\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 2 Dataset 4:\n",
            "  Accuracy: 0.4476\n",
            "  Uncertainty: 0.0110\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 2 Dataset 5:\n",
            "  Accuracy: 0.5952\n",
            "  Uncertainty: 0.0113\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 2 Dataset 6:\n",
            "  Accuracy: 0.4784\n",
            "  Uncertainty: 0.0142\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 9 on Part 2 Dataset 7:\n",
            "  Accuracy: 0.4544\n",
            "  Uncertainty: 0.0120\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining on sequential datasets:  80%|  | 8/10 [28:52<07:57, 238.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest Model 9 on Part 2 Dataset 8:\n",
            "  Accuracy: 0.4680\n",
            "  Uncertainty: 0.0139\n",
            "\n",
            "Processing dataset 19\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/train_data/9_train_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Using 1875/2500 confident predictions\n",
            "\n",
            "Evaluating latest model...\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 1 Dataset 1:\n",
            "  Accuracy: 0.5924\n",
            "  Uncertainty: 0.0130\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 1 Dataset 2:\n",
            "  Accuracy: 0.5832\n",
            "  Uncertainty: 0.0131\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 1 Dataset 3:\n",
            "  Accuracy: 0.5768\n",
            "  Uncertainty: 0.0117\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 1 Dataset 4:\n",
            "  Accuracy: 0.5804\n",
            "  Uncertainty: 0.0149\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 1 Dataset 5:\n",
            "  Accuracy: 0.5872\n",
            "  Uncertainty: 0.0112\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 1 Dataset 6:\n",
            "  Accuracy: 0.5816\n",
            "  Uncertainty: 0.0109\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 1 Dataset 7:\n",
            "  Accuracy: 0.5700\n",
            "  Uncertainty: 0.0130\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 1 Dataset 8:\n",
            "  Accuracy: 0.5780\n",
            "  Uncertainty: 0.0144\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 1 Dataset 9:\n",
            "  Accuracy: 0.5764\n",
            "  Uncertainty: 0.0116\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 1 Dataset 10:\n",
            "  Accuracy: 0.5788\n",
            "  Uncertainty: 0.0144\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 2 Dataset 1:\n",
            "  Accuracy: 0.5380\n",
            "  Uncertainty: 0.0138\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 2 Dataset 2:\n",
            "  Accuracy: 0.4492\n",
            "  Uncertainty: 0.0153\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 2 Dataset 3:\n",
            "  Accuracy: 0.5016\n",
            "  Uncertainty: 0.0128\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 2 Dataset 4:\n",
            "  Accuracy: 0.4460\n",
            "  Uncertainty: 0.0113\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 2 Dataset 5:\n",
            "  Accuracy: 0.5940\n",
            "  Uncertainty: 0.0115\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 2 Dataset 6:\n",
            "  Accuracy: 0.4780\n",
            "  Uncertainty: 0.0144\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 2 Dataset 7:\n",
            "  Accuracy: 0.4536\n",
            "  Uncertainty: 0.0118\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 10 on Part 2 Dataset 8:\n",
            "  Accuracy: 0.4672\n",
            "  Uncertainty: 0.0140\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining on sequential datasets:  90%| | 9/10 [33:26<04:09, 249.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest Model 10 on Part 2 Dataset 9:\n",
            "  Accuracy: 0.5096\n",
            "  Uncertainty: 0.0124\n",
            "\n",
            "Processing dataset 20\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/train_data/10_train_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Using 1875/2500 confident predictions\n",
            "\n",
            "Evaluating latest model...\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 1 Dataset 1:\n",
            "  Accuracy: 0.5920\n",
            "  Uncertainty: 0.0133\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 1 Dataset 2:\n",
            "  Accuracy: 0.5836\n",
            "  Uncertainty: 0.0128\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 1 Dataset 3:\n",
            "  Accuracy: 0.5768\n",
            "  Uncertainty: 0.0118\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 1 Dataset 4:\n",
            "  Accuracy: 0.5812\n",
            "  Uncertainty: 0.0148\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 1 Dataset 5:\n",
            "  Accuracy: 0.5872\n",
            "  Uncertainty: 0.0110\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 1 Dataset 6:\n",
            "  Accuracy: 0.5816\n",
            "  Uncertainty: 0.0110\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 1 Dataset 7:\n",
            "  Accuracy: 0.5708\n",
            "  Uncertainty: 0.0131\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 1 Dataset 8:\n",
            "  Accuracy: 0.5800\n",
            "  Uncertainty: 0.0145\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 1 Dataset 9:\n",
            "  Accuracy: 0.5764\n",
            "  Uncertainty: 0.0116\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 1 Dataset 10:\n",
            "  Accuracy: 0.5796\n",
            "  Uncertainty: 0.0144\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/1_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 2 Dataset 1:\n",
            "  Accuracy: 0.5380\n",
            "  Uncertainty: 0.0139\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/2_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 2 Dataset 2:\n",
            "  Accuracy: 0.4472\n",
            "  Uncertainty: 0.0156\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/3_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 2 Dataset 3:\n",
            "  Accuracy: 0.5008\n",
            "  Uncertainty: 0.0128\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/4_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 2 Dataset 4:\n",
            "  Accuracy: 0.4460\n",
            "  Uncertainty: 0.0114\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/5_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 2 Dataset 5:\n",
            "  Accuracy: 0.5944\n",
            "  Uncertainty: 0.0115\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/6_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 2 Dataset 6:\n",
            "  Accuracy: 0.4780\n",
            "  Uncertainty: 0.0147\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/7_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 2 Dataset 7:\n",
            "  Accuracy: 0.4532\n",
            "  Uncertainty: 0.0119\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/8_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 2 Dataset 8:\n",
            "  Accuracy: 0.4668\n",
            "  Uncertainty: 0.0141\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/9_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n",
            "Latest Model 11 on Part 2 Dataset 9:\n",
            "  Accuracy: 0.5092\n",
            "  Uncertainty: 0.0122\n",
            "Loaded data from /content/drive/MyDrive/dataset/part_two_dataset/eval_data/10_eval_data.tar.pth: torch.Size([2500, 32, 32, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training on sequential datasets: 100%|| 10/10 [38:12<00:00, 229.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest Model 11 on Part 2 Dataset 10:\n",
            "  Accuracy: 0.5284\n",
            "  Uncertainty: 0.0134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}